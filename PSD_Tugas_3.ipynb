{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## 1. Import Library yang Diperlukan\n",
    "\n",
    "Pada tahap awal, kita perlu mengimpor semua library yang akan digunakan dalam proses data wrangling. Library ini mencakup tools untuk manipulasi data, visualisasi, dan text processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library untuk manipulasi dan analisis data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Library untuk visualisasi data\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Library untuk text processing dan regular expressions\n",
    "import re\n",
    "\n",
    "# Library untuk machine learning dan split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Library untuk deep learning dengan transformers\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Konfigurasi untuk tampilan yang lebih baik\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "print(\"Semua library berhasil diimpor.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## 2. Load Dataset\n",
    "\n",
    "Tahap pertama dalam data wrangling adalah memuat dataset dari sumber yang tersedia. Dataset ini diambil dari GitHub repository dalam format CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memuat dataset dari URL GitHub\n",
    "try:\n",
    "    df = pd.read_csv('https://github.com/erlanggadewasakti/Prinsip-Sains-Data/releases/download/prod/sa-psd-dataset.csv')\n",
    "    print(f\"Dataset berhasil dimuat dengan {len(df)} baris data.\")\n",
    "    print(f\"Jumlah kolom: {len(df.columns)}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: File dataset tidak ditemukan. Pastikan URL sudah benar.\")\n",
    "except Exception as e:\n",
    "    print(f\"Terjadi error saat membaca file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 3. Inspeksi Awal Dataset\n",
    "\n",
    "Sebelum melakukan cleaning, kita perlu memahami struktur dan karakteristik dataset. Ini termasuk melihat sample data, tipe data, dan informasi statistik dasar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menampilkan 5 baris pertama dataset\n",
    "print(\"=\" * 80)\n",
    "print(\"SAMPLE DATA (5 baris pertama)\")\n",
    "print(\"=\" * 80)\n",
    "display(df.head())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"INFORMASI STRUKTUR DATASET\")\n",
    "print(\"=\" * 80)\n",
    "df.info()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STATISTIK DESKRIPTIF\")\n",
    "print(\"=\" * 80)\n",
    "display(df.describe(include='all'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## 4. Identifikasi dan Penanganan Missing Values\n",
    "\n",
    "Missing values bisa mempengaruhi kualitas analisis dan model. Kita perlu mengidentifikasi kolom mana yang memiliki nilai kosong dan menentukan strategi penanganannya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mengidentifikasi missing values\n",
    "print(\"=\" * 80)\n",
    "print(\"ANALISIS MISSING VALUES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percentage = (df.isnull().sum() / len(df)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Kolom': missing_values.index,\n",
    "    'Jumlah Missing': missing_values.values,\n",
    "    'Persentase (%)': missing_percentage.values\n",
    "})\n",
    "\n",
    "print(missing_df[missing_df['Jumlah Missing'] > 0])\n",
    "\n",
    "if missing_df['Jumlah Missing'].sum() == 0:\n",
    "    print(\"\\nTidak ditemukan missing values dalam dataset.\")\n",
    "else:\n",
    "    print(f\"\\nTotal missing values: {missing_df['Jumlah Missing'].sum()}\")\n",
    "\n",
    "# Visualisasi missing values jika ada\n",
    "if missing_df['Jumlah Missing'].sum() > 0:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    missing_cols = missing_df[missing_df['Jumlah Missing'] > 0]\n",
    "    plt.barh(missing_cols['Kolom'], missing_cols['Persentase (%)'])\n",
    "    plt.xlabel('Persentase Missing Values (%)')\n",
    "    plt.title('Distribusi Missing Values per Kolom')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menangani missing values jika ada\n",
    "# Strategi: Drop baris dengan missing values pada kolom penting (input dan output)\n",
    "# karena kedua kolom ini essential untuk analisis sentimen\n",
    "\n",
    "df_clean = df.copy()\n",
    "\n",
    "if df_clean.isnull().sum().sum() > 0:\n",
    "    # Drop baris yang memiliki missing values pada kolom input atau output\n",
    "    initial_rows = len(df_clean)\n",
    "    df_clean = df_clean.dropna(subset=['input', 'output'], how='any')\n",
    "    dropped_rows = initial_rows - len(df_clean)\n",
    "\n",
    "    print(f\"Jumlah baris yang dihapus karena missing values: {dropped_rows}\")\n",
    "    print(f\"Sisa data setelah penanganan missing values: {len(df_clean)} baris\")\n",
    "else:\n",
    "    print(\"Tidak ada missing values yang perlu ditangani.\")\n",
    "\n",
    "# Verifikasi tidak ada lagi missing values\n",
    "print(f\"\\nVerifikasi - Total missing values setelah cleaning: {df_clean.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## 5. Identifikasi dan Penanganan Data Duplikat\n",
    "\n",
    "Data duplikat dapat menyebabkan bias dalam analisis dan model. Kita perlu mengidentifikasi dan menghapus baris yang benar-benar duplikat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mengidentifikasi duplikasi\n",
    "print(\"=\" * 80)\n",
    "print(\"ANALISIS DATA DUPLIKAT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Cek duplikasi pada semua kolom\n",
    "duplicates_all = df_clean.duplicated().sum()\n",
    "print(f\"Jumlah baris duplikat (semua kolom): {duplicates_all}\")\n",
    "\n",
    "# Cek duplikasi spesifik pada kolom 'input' karena ini adalah text utama\n",
    "duplicates_input = df_clean.duplicated(subset=['input']).sum()\n",
    "print(f\"Jumlah duplikasi pada kolom 'input': {duplicates_input}\")\n",
    "\n",
    "# Tampilkan contoh data duplikat jika ada\n",
    "if duplicates_input > 0:\n",
    "    print(\"\\nContoh data duplikat (berdasarkan kolom 'input'):\")\n",
    "    duplicate_samples = df_clean[df_clean.duplicated(subset=['input'], keep=False)].sort_values('input').head(10)\n",
    "    display(duplicate_samples[['input', 'output']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menghapus duplikasi\n",
    "initial_rows = len(df_clean)\n",
    "\n",
    "# Hapus duplikat berdasarkan kolom 'input', keep='first' untuk mempertahankan kemunculan pertama\n",
    "df_clean = df_clean.drop_duplicates(subset=['input'], keep='first')\n",
    "\n",
    "removed_duplicates = initial_rows - len(df_clean)\n",
    "\n",
    "print(f\"Jumlah baris duplikat yang dihapus: {removed_duplicates}\")\n",
    "print(f\"Total data setelah menghapus duplikasi: {len(df_clean)} baris\")\n",
    "\n",
    "# Reset index setelah menghapus duplikasi\n",
    "df_clean = df_clean.reset_index(drop=True)\n",
    "print(f\"Index berhasil direset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## 6. Text Cleaning dan Preprocessing\n",
    "\n",
    "Tahap ini melakukan pembersihan teks untuk memastikan data siap diproses oleh model. Proses ini menghapus noise seperti URL, HTML tags, karakter khusus, dan menormalisasi whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_text_cleaning(text):\n",
    "    \"\"\"\n",
    "    Fungsi untuk membersihkan teks dari berbagai noise dan inkonsistensi.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    text : str\n",
    "        Teks yang akan dibersihkan\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    str\n",
    "        Teks yang sudah dibersihkan\n",
    "\n",
    "    Proses cleaning meliputi:\n",
    "    1. Menghapus URL (http, https, www)\n",
    "    2. Menghapus HTML tags\n",
    "    3. Menghapus karakter khusus (hanya menyisakan huruf, angka, dan spasi)\n",
    "    4. Normalisasi whitespace (mengganti multiple spaces dengan single space)\n",
    "    \"\"\"\n",
    "\n",
    "    # Handle input yang bukan string\n",
    "    if not isinstance(text, str):\n",
    "        return str(text) if text is not None else \"\"\n",
    "\n",
    "    # Hapus URL dengan pattern matching\n",
    "    # Pattern ini menangkap URL yang dimulai dengan http://, https://, atau www.\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # Hapus HTML tags menggunakan regex\n",
    "    # Pattern <.*?> menangkap semua tag HTML\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "\n",
    "    # Hapus karakter khusus, hanya pertahankan huruf (a-z, A-Z), angka (0-9), dan spasi\n",
    "    # Ini membantu mengurangi dimensionalitas dan noise dalam data\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "\n",
    "    # Normalisasi whitespace: ganti multiple spaces dengan single space\n",
    "    # dan hapus leading/trailing spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# Aplikasikan fungsi cleaning ke kolom 'input'\n",
    "print(\"Memulai proses text cleaning...\")\n",
    "df_clean['cleaned_input'] = df_clean['input'].apply(advanced_text_cleaning)\n",
    "\n",
    "# Tampilkan perbandingan sebelum dan sesudah cleaning\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PERBANDINGAN TEKS SEBELUM DAN SESUDAH CLEANING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "comparison_df = df_clean[['input', 'cleaned_input']].head()\n",
    "for idx, row in comparison_df.iterrows():\n",
    "    print(f\"\\n--- Sample {idx + 1} ---\")\n",
    "    print(f\"Original  : {row['input'][:150]}...\")\n",
    "    print(f\"Cleaned   : {row['cleaned_input'][:150]}...\")\n",
    "\n",
    "print(f\"\\nText cleaning selesai untuk {len(df_clean)} baris data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## 7. Ekstraksi dan Transformasi Label Sentimen\n",
    "\n",
    "Pada tahap ini, kita mengekstrak label sentimen dari kolom output dan melakukan normalisasi kategori sentimen (menggabungkan 'very positive' dengan 'positive' dan 'very negative' dengan 'negative')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ekstraksi label sentimen dari kolom output\n",
    "# Format awal: \"A: very positive\", \"B: positive\", dst.\n",
    "# Kita hapus prefix (A:, B:, dst) untuk mendapatkan label yang bersih\n",
    "\n",
    "df_clean['sentiment'] = df_clean['output'].str.replace(r'^[A-E]:\\s*', '', regex=True)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DISTRIBUSI SENTIMEN SEBELUM NORMALISASI\")\n",
    "print(\"=\" * 80)\n",
    "print(df_clean['sentiment'].value_counts())\n",
    "\n",
    "# Normalisasi label sentimen\n",
    "# Menggabungkan kategori yang sangat mirip untuk menyederhanakan klasifikasi\n",
    "# very positive -> positive, very negative -> negative\n",
    "sentiment_normalization = {\n",
    "    'very positive': 'positive',\n",
    "    'very negative': 'negative',\n",
    "    'positive': 'positive',\n",
    "    'negative': 'negative',\n",
    "    'neutral': 'neutral'\n",
    "}\n",
    "\n",
    "df_clean['sentiment'] = df_clean['sentiment'].map(sentiment_normalization)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DISTRIBUSI SENTIMEN SETELAH NORMALISASI\")\n",
    "print(\"=\" * 80)\n",
    "print(df_clean['sentiment'].value_counts())\n",
    "print(\"\\nProporsi masing-masing kelas:\")\n",
    "print(df_clean['sentiment'].value_counts(normalize=True))\n",
    "\n",
    "# Tampilkan sample hasil ekstraksi\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SAMPLE HASIL EKSTRAKSI SENTIMEN\")\n",
    "print(\"=\" * 80)\n",
    "display(df_clean[['output', 'sentiment', 'cleaned_input']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## 8. Visualisasi Distribusi Sentimen\n",
    "\n",
    "Visualisasi membantu kita memahami balance/imbalance dari kelas sentimen dalam dataset. Ini penting untuk menentukan apakah perlu dilakukan teknik balancing seperti oversampling atau undersampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konfigurasi warna untuk setiap sentimen agar visualisasi lebih informatif\n",
    "sentiment_colors = {\n",
    "    'positive': '#2ecc71',  # Hijau untuk sentimen positif\n",
    "    'negative': '#e74c3c',  # Merah untuk sentimen negatif\n",
    "    'neutral': '#3498db'    # Biru untuk sentimen netral\n",
    "}\n",
    "\n",
    "# Membuat visualisasi distribusi sentimen\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Count plot\n",
    "sns.countplot(\n",
    "    x='sentiment',\n",
    "    data=df_clean,\n",
    "    hue='sentiment',\n",
    "    palette=sentiment_colors,\n",
    "    legend=False,\n",
    "    ax=axes[0]\n",
    ")\n",
    "axes[0].set_title('Distribusi Sentimen (Count)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Kategori Sentimen', fontsize=12)\n",
    "axes[0].set_ylabel('Jumlah Data', fontsize=12)\n",
    "\n",
    "# Tambahkan nilai di atas setiap bar\n",
    "for container in axes[0].containers:\n",
    "    axes[0].bar_label(container, fmt='%d')\n",
    "\n",
    "# Plot 2: Pie chart untuk proporsi\n",
    "sentiment_counts = df_clean['sentiment'].value_counts()\n",
    "colors = [sentiment_colors[label] for label in sentiment_counts.index]\n",
    "\n",
    "axes[1].pie(\n",
    "    sentiment_counts.values,\n",
    "    labels=sentiment_counts.index,\n",
    "    autopct='%1.1f%%',\n",
    "    colors=colors,\n",
    "    startangle=90,\n",
    "    textprops={'fontsize': 11}\n",
    ")\n",
    "axes[1].set_title('Proporsi Sentimen', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analisis class imbalance\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ANALISIS CLASS IMBALANCE\")\n",
    "print(\"=\" * 80)\n",
    "majority_class = sentiment_counts.idxmax()\n",
    "minority_class = sentiment_counts.idxmin()\n",
    "imbalance_ratio = sentiment_counts.max() / sentiment_counts.min()\n",
    "\n",
    "print(f\"Kelas mayoritas: {majority_class} ({sentiment_counts.max()} data)\")\n",
    "print(f\"Kelas minoritas: {minority_class} ({sentiment_counts.min()} data)\")\n",
    "print(f\"Rasio imbalance: {imbalance_ratio:.2f}:1\")\n",
    "\n",
    "if imbalance_ratio > 3:\n",
    "    print(\"\\nâš ï¸  PERINGATAN: Dataset mengalami class imbalance yang signifikan!\")\n",
    "    print(\"   Rekomendasi: Gunakan teknik oversampling atau class weighting saat training model.\")\n",
    "else:\n",
    "    print(\"\\nâœ“ Dataset relatif seimbang.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## 9. Encoding Label Kategorikal\n",
    "\n",
    "Machine learning model memerlukan input numerik. Oleh karena itu, kita perlu mengubah label sentimen kategorikal menjadi representasi numerik menggunakan label encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definisi mapping untuk encoding sentimen\n",
    "# Mapping ini konsisten dan akan digunakan sepanjang proses\n",
    "sentiment_mapping = {\n",
    "    'positive': 0,\n",
    "    'neutral': 1,\n",
    "    'negative': 2\n",
    "}\n",
    "\n",
    "# Aplikasikan encoding\n",
    "df_clean['sentiment_encoded'] = df_clean['sentiment'].map(sentiment_mapping)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SENTIMENT ENCODING MAPPING\")\n",
    "print(\"=\" * 80)\n",
    "for sentiment, code in sentiment_mapping.items():\n",
    "    count = (df_clean['sentiment_encoded'] == code).sum()\n",
    "    print(f\"{sentiment:10s} -> {code} ({count:6d} data)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SAMPLE DATA DENGAN ENCODED SENTIMEN\")\n",
    "print(\"=\" * 80)\n",
    "display(df_clean[['cleaned_input', 'sentiment', 'sentiment_encoded']].head(10))\n",
    "\n",
    "# Verifikasi tidak ada missing values setelah encoding\n",
    "assert df_clean['sentiment_encoded'].isnull().sum() == 0, \"Terdapat nilai yang tidak termap dalam encoding!\"\n",
    "print(\"\\nâœ“ Encoding berhasil, tidak ada nilai yang terlewat.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## 10. Analisis Panjang Teks\n",
    "\n",
    "Sebelum split data, kita perlu memahami distribusi panjang teks. Ini membantu dalam menentukan parameter seperti max_length untuk tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hitung panjang teks (jumlah karakter dan jumlah kata)\n",
    "df_clean['text_length_chars'] = df_clean['cleaned_input'].str.len()\n",
    "df_clean['text_length_words'] = df_clean['cleaned_input'].str.split().str.len()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STATISTIK PANJANG TEKS\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nBerdasarkan jumlah karakter:\")\n",
    "print(df_clean['text_length_chars'].describe())\n",
    "\n",
    "print(\"\\nBerdasarkan jumlah kata:\")\n",
    "print(df_clean['text_length_words'].describe())\n",
    "\n",
    "# Visualisasi distribusi panjang teks\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Histogram panjang karakter\n",
    "axes[0].hist(df_clean['text_length_chars'], bins=50, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(df_clean['text_length_chars'].median(), color='red', linestyle='--',\n",
    "                label=f\"Median: {df_clean['text_length_chars'].median():.0f}\")\n",
    "axes[0].set_xlabel('Jumlah Karakter', fontsize=12)\n",
    "axes[0].set_ylabel('Frekuensi', fontsize=12)\n",
    "axes[0].set_title('Distribusi Panjang Teks (Karakter)', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "\n",
    "# Histogram panjang kata\n",
    "axes[1].hist(df_clean['text_length_words'], bins=50, color='lightcoral', edgecolor='black', alpha=0.7)\n",
    "axes[1].axvline(df_clean['text_length_words'].median(), color='blue', linestyle='--',\n",
    "                label=f\"Median: {df_clean['text_length_words'].median():.0f}\")\n",
    "axes[1].set_xlabel('Jumlah Kata', fontsize=12)\n",
    "axes[1].set_ylabel('Frekuensi', fontsize=12)\n",
    "axes[1].set_title('Distribusi Panjang Teks (Kata)', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "## 11. Split Data: Training, Validation, dan Testing\n",
    "\n",
    "Data split dilakukan dengan proporsi 70% training, 15% validation, dan 15% testing. Stratifikasi digunakan untuk memastikan distribusi sentimen yang konsisten di setiap set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konstanta untuk reproducibility\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.3  # 30% untuk gabungan validation + testing\n",
    "VAL_TEST_SPLIT = 0.5  # 50% dari 30% = 15% untuk masing-masing validation dan testing\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PROSES SPLIT DATA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Step 1: Split menjadi training (70%) dan temporary set (30%)\n",
    "df_train, df_temp, _, _ = train_test_split(\n",
    "    df_clean,\n",
    "    df_clean['sentiment_encoded'],\n",
    "    test_size=TEST_SIZE,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=df_clean['sentiment_encoded']\n",
    ")\n",
    "\n",
    "print(f\"Step 1: Split data menjadi training (70%) dan temporary (30%)\")\n",
    "print(f\"  - Training set: {len(df_train)} baris\")\n",
    "print(f\"  - Temporary set: {len(df_temp)} baris\")\n",
    "\n",
    "# Step 2: Split temporary set menjadi validation (15%) dan testing (15%)\n",
    "# test_size=0.5 karena 0.5 * 30% = 15% dari dataset original\n",
    "df_val, df_test, _, _ = train_test_split(\n",
    "    df_temp,\n",
    "    df_temp['sentiment_encoded'],\n",
    "    test_size=VAL_TEST_SPLIT,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=df_temp['sentiment_encoded']\n",
    ")\n",
    "\n",
    "print(f\"\\nStep 2: Split temporary set menjadi validation (15%) dan testing (15%)\")\n",
    "print(f\"  - Validation set: {len(df_val)} baris\")\n",
    "print(f\"  - Testing set: {len(df_test)} baris\")\n",
    "\n",
    "# Verifikasi proporsi\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"VERIFIKASI PROPORSI SPLIT\")\n",
    "print(\"=\" * 80)\n",
    "total_data = len(df_clean)\n",
    "print(f\"Total data original: {total_data}\")\n",
    "print(f\"Training   : {len(df_train):6d} baris ({len(df_train)/total_data*100:.1f}%)\")\n",
    "print(f\"Validation : {len(df_val):6d} baris ({len(df_val)/total_data*100:.1f}%)\")\n",
    "print(f\"Testing    : {len(df_test):6d} baris ({len(df_test)/total_data*100:.1f}%)\")\n",
    "print(f\"Total      : {len(df_train) + len(df_val) + len(df_test):6d} baris\")\n",
    "\n",
    "# Verifikasi stratifikasi - distribusi sentimen harus konsisten\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"VERIFIKASI STRATIFIKASI - DISTRIBUSI SENTIMEN\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nTraining Set:\")\n",
    "print(df_train['sentiment'].value_counts(normalize=True).sort_index())\n",
    "\n",
    "print(\"\\nValidation Set:\")\n",
    "print(df_val['sentiment'].value_counts(normalize=True).sort_index())\n",
    "\n",
    "print(\"\\nTesting Set:\")\n",
    "print(df_test['sentiment'].value_counts(normalize=True).sort_index())\n",
    "\n",
    "print(\"\\nâœ“ Stratifikasi berhasil - distribusi sentimen konsisten di setiap set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "## 12. Oversampling pada Training Set\n",
    "\n",
    "Class imbalance dapat menyebabkan model bias terhadap kelas mayoritas. Untuk mengatasi ini, kita lakukan oversampling dengan teknik random sampling dengan replacement pada kelas minoritas.\n",
    "\n",
    "**Catatan Penting**: Oversampling hanya dilakukan pada training set untuk mencegah data leakage. Validation dan testing set dibiarkan natural untuk evaluasi yang fair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"OVERSAMPLING TRAINING SET\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Identifikasi kelas mayoritas dan jumlahnya\n",
    "majority_count = df_train['sentiment'].value_counts().max()\n",
    "majority_class = df_train['sentiment'].value_counts().idxmax()\n",
    "\n",
    "print(f\"Kelas mayoritas: {majority_class}\")\n",
    "print(f\"Target jumlah per kelas: {majority_count} samples\\n\")\n",
    "\n",
    "print(\"Distribusi SEBELUM oversampling:\")\n",
    "print(df_train['sentiment'].value_counts().sort_index())\n",
    "\n",
    "# Inisialisasi dataframe untuk hasil oversampling\n",
    "df_train_oversampled = pd.DataFrame()\n",
    "\n",
    "# Lakukan oversampling untuk setiap kelas sentimen\n",
    "for sentiment_label in df_train['sentiment'].unique():\n",
    "    # Ambil data untuk sentimen tertentu\n",
    "    sentiment_df = df_train[df_train['sentiment'] == sentiment_label]\n",
    "    current_count = len(sentiment_df)\n",
    "\n",
    "    if current_count < majority_count:\n",
    "        # Kelas minoritas: lakukan oversampling dengan random sampling (replacement=True)\n",
    "        # Ini akan menduplikasi beberapa sample secara random hingga mencapai jumlah mayoritas\n",
    "        oversampled_df = sentiment_df.sample(\n",
    "            n=majority_count,\n",
    "            replace=True,\n",
    "            random_state=RANDOM_STATE\n",
    "        )\n",
    "        print(f\"  {sentiment_label:10s}: {current_count:5d} -> {len(oversampled_df):5d} samples (oversampled)\")\n",
    "    else:\n",
    "        # Kelas mayoritas: gunakan semua data tanpa perubahan\n",
    "        oversampled_df = sentiment_df\n",
    "        print(f\"  {sentiment_label:10s}: {current_count:5d} samples (unchanged)\")\n",
    "\n",
    "    # Gabungkan ke dataframe hasil\n",
    "    df_train_oversampled = pd.concat([df_train_oversampled, oversampled_df], ignore_index=True)\n",
    "\n",
    "# Shuffle data setelah oversampling untuk menghindari ordering bias\n",
    "df_train_oversampled = df_train_oversampled.sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "\n",
    "print(\"\\nDistribusi SETELAH oversampling:\")\n",
    "print(df_train_oversampled['sentiment'].value_counts().sort_index())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SUMMARY OVERSAMPLING\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Ukuran training set sebelum oversampling: {len(df_train):6d} baris\")\n",
    "print(f\"Ukuran training set setelah oversampling: {len(df_train_oversampled):6d} baris\")\n",
    "print(f\"Peningkatan: {len(df_train_oversampled) - len(df_train):6d} baris ({(len(df_train_oversampled)/len(df_train) - 1)*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nâœ“ Oversampling selesai - semua kelas memiliki jumlah sample yang seimbang.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "## 13. Visualisasi Perbandingan Sebelum dan Sesudah Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat visualisasi perbandingan\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot sebelum oversampling\n",
    "df_train['sentiment'].value_counts().sort_index().plot(\n",
    "    kind='bar',\n",
    "    ax=axes[0],\n",
    "    color=['#2ecc71', '#e74c3c', '#3498db'],\n",
    "    edgecolor='black',\n",
    "    alpha=0.7\n",
    ")\n",
    "axes[0].set_title('Distribusi Sentimen SEBELUM Oversampling', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Kategori Sentimen', fontsize=12)\n",
    "axes[0].set_ylabel('Jumlah Data', fontsize=12)\n",
    "axes[0].set_xticklabels(['Negative', 'Neutral', 'Positive'], rotation=0)\n",
    "\n",
    "# Tambahkan nilai di atas bar\n",
    "for container in axes[0].containers:\n",
    "    axes[0].bar_label(container, fmt='%d')\n",
    "\n",
    "# Plot setelah oversampling\n",
    "df_train_oversampled['sentiment'].value_counts().sort_index().plot(\n",
    "    kind='bar',\n",
    "    ax=axes[1],\n",
    "    color=['#2ecc71', '#e74c3c', '#3498db'],\n",
    "    edgecolor='black',\n",
    "    alpha=0.7\n",
    ")\n",
    "axes[1].set_title('Distribusi Sentimen SETELAH Oversampling', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Kategori Sentimen', fontsize=12)\n",
    "axes[1].set_ylabel('Jumlah Data', fontsize=12)\n",
    "axes[1].set_xticklabels(['Negative', 'Neutral', 'Positive'], rotation=0)\n",
    "\n",
    "# Tambahkan nilai di atas bar\n",
    "for container in axes[1].containers:\n",
    "    axes[1].bar_label(container, fmt='%d')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## 14. Tokenisasi dengan BERT Tokenizer\n",
    "\n",
    "Untuk model berbasis transformer seperti BERT, kita perlu mengubah teks menjadi token numerik. Proses ini menggunakan pre-trained BERT tokenizer yang sudah dilatih pada vocabulary yang luas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained BERT tokenizer\n",
    "# bert-base-uncased: versi BERT dengan lowercase text\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"BERT TOKENIZER\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Model tokenizer: bert-base-uncased\")\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size:,} tokens\")\n",
    "print(f\"Model max length: {tokenizer.model_max_length} tokens\")\n",
    "print(\"\\nâœ“ BERT Tokenizer berhasil dimuat.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "## 15. Analisis Distribusi Panjang Token\n",
    "\n",
    "Sebelum menentukan max_length untuk tokenization, kita perlu menganalisis distribusi panjang token dalam data. Ini membantu kita menemukan nilai optimal yang bisa menangani sebagian besar data tanpa terlalu banyak padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hitung panjang token untuk setiap teks dalam training set yang sudah di-oversample\n",
    "# Kita gunakan truncation di sini hanya untuk analisis, bukan untuk tokenization final\n",
    "print(\"Menghitung panjang token untuk analisis distribusi...\")\n",
    "\n",
    "token_lengths = [\n",
    "    len(tokenizer.encode(\n",
    "        str(text),\n",
    "        add_special_tokens=True,\n",
    "        truncation=True,\n",
    "        max_length=tokenizer.model_max_length\n",
    "    ))\n",
    "    for text in df_train_oversampled['cleaned_input']\n",
    "]\n",
    "\n",
    "print(f\"Selesai menghitung {len(token_lengths)} samples.\\n\")\n",
    "\n",
    "# Statistik deskriptif\n",
    "print(\"=\" * 80)\n",
    "print(\"STATISTIK PANJANG TOKEN\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Minimum      : {np.min(token_lengths)} tokens\")\n",
    "print(f\"Maksimum     : {np.max(token_lengths)} tokens\")\n",
    "print(f\"Mean         : {np.mean(token_lengths):.2f} tokens\")\n",
    "print(f\"Median       : {np.median(token_lengths):.0f} tokens\")\n",
    "print(f\"Std Dev      : {np.std(token_lengths):.2f} tokens\")\n",
    "print(f\"Percentile 90: {np.percentile(token_lengths, 90):.0f} tokens\")\n",
    "print(f\"Percentile 95: {np.percentile(token_lengths, 95):.0f} tokens\")\n",
    "print(f\"Percentile 99: {np.percentile(token_lengths, 99):.0f} tokens\")\n",
    "\n",
    "# Visualisasi distribusi\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(token_lengths, bins=50, kde=True, color='steelblue', edgecolor='black', alpha=0.6)\n",
    "\n",
    "# Tambahkan garis vertikal untuk statistik penting\n",
    "plt.axvline(np.mean(token_lengths), color='red', linestyle='--',\n",
    "            label=f'Mean: {np.mean(token_lengths):.0f}', linewidth=2)\n",
    "plt.axvline(np.percentile(token_lengths, 95), color='orange', linestyle='--',\n",
    "            label=f'95th Percentile: {np.percentile(token_lengths, 95):.0f}', linewidth=2)\n",
    "plt.axvline(np.percentile(token_lengths, 99), color='green', linestyle='--',\n",
    "            label=f'99th Percentile: {np.percentile(token_lengths, 99):.0f}', linewidth=2)\n",
    "\n",
    "plt.xlabel('Jumlah Token', fontsize=12)\n",
    "plt.ylabel('Frekuensi', fontsize=12)\n",
    "plt.title('Distribusi Panjang Token pada Training Set (Oversampled)',\n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Tentukan MAX_LENGTH optimal\n",
    "# Kita gunakan percentile 95 atau 99 untuk coverage yang baik tanpa terlalu banyak padding\n",
    "# Namun tetap dibatasi maksimal 512 (limit BERT)\n",
    "suggested_max_length = int(np.percentile(token_lengths, 95))\n",
    "MAX_LENGTH = min(suggested_max_length, 512)\n",
    "\n",
    "# Untuk efisiensi, kita bisa set ke nilai yang lebih reasonable jika terlalu panjang\n",
    "if MAX_LENGTH > 400:\n",
    "    MAX_LENGTH = 330  # Nilai praktis yang masih cover majority data\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PENENTUAN MAX_LENGTH\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Suggested max_length (95th percentile): {suggested_max_length} tokens\")\n",
    "print(f\"Chosen MAX_LENGTH: {MAX_LENGTH} tokens\")\n",
    "print(f\"Coverage: ~{(np.array(token_lengths) <= MAX_LENGTH).sum() / len(token_lengths) * 100:.1f}% dari data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "## 16. Tokenisasi Final pada Semua Dataset Split\n",
    "\n",
    "Dengan MAX_LENGTH yang sudah ditentukan, kita lakukan tokenisasi pada training, validation, dan testing set. Proses ini mengkonversi teks menjadi format yang siap digunakan oleh model BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(texts, tokenizer, max_length):\n",
    "    \"\"\"\n",
    "    Fungsi untuk melakukan tokenisasi pada sekumpulan teks.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    texts : Series/List\n",
    "        Koleksi teks yang akan ditokenisasi\n",
    "    tokenizer : PreTrainedTokenizer\n",
    "        BERT tokenizer\n",
    "    max_length : int\n",
    "        Panjang maksimal sequence setelah tokenization\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    BatchEncoding\n",
    "        Dictionary berisi input_ids, attention_mask, dan token_type_ids dalam format PyTorch tensor\n",
    "\n",
    "    Process:\n",
    "    --------\n",
    "    - padding='max_length': Tambahkan padding hingga max_length\n",
    "    - truncation=True: Potong teks yang melebihi max_length\n",
    "    - return_tensors='pt': Return dalam format PyTorch tensor\n",
    "    \"\"\"\n",
    "    return tokenizer(\n",
    "        list(texts),\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TOKENISASI DATASET\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Tokenisasi training set (yang sudah di-oversample)\n",
    "print(\"Melakukan tokenisasi pada training set (oversampled)...\")\n",
    "X_train_tokenized = tokenize_data(df_train_oversampled['cleaned_input'], tokenizer, MAX_LENGTH)\n",
    "print(f\"  âœ“ Selesai - Shape: {X_train_tokenized['input_ids'].shape}\")\n",
    "\n",
    "# Tokenisasi validation set\n",
    "print(\"Melakukan tokenisasi pada validation set...\")\n",
    "X_val_tokenized = tokenize_data(df_val['cleaned_input'], tokenizer, MAX_LENGTH)\n",
    "print(f\"  âœ“ Selesai - Shape: {X_val_tokenized['input_ids'].shape}\")\n",
    "\n",
    "# Tokenisasi testing set\n",
    "print(\"Melakukan tokenisasi pada testing set...\")\n",
    "X_test_tokenized = tokenize_data(df_test['cleaned_input'], tokenizer, MAX_LENGTH)\n",
    "print(f\"  âœ“ Selesai - Shape: {X_test_tokenized['input_ids'].shape}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SUMMARY TOKENISASI\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"MAX_LENGTH yang digunakan: {MAX_LENGTH} tokens\")\n",
    "print(f\"\\nTraining set   : {X_train_tokenized['input_ids'].shape[0]:6d} samples Ã— {MAX_LENGTH} tokens\")\n",
    "print(f\"Validation set : {X_val_tokenized['input_ids'].shape[0]:6d} samples Ã— {MAX_LENGTH} tokens\")\n",
    "print(f\"Testing set    : {X_test_tokenized['input_ids'].shape[0]:6d} samples Ã— {MAX_LENGTH} tokens\")\n",
    "\n",
    "print(\"\\nâœ“ Tokenisasi berhasil untuk semua dataset split.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "## 17. Konversi ke PyTorch Tensors\n",
    "\n",
    "Model deep learning memerlukan input dalam format tensor. Kita konversi hasil tokenisasi dan label menjadi PyTorch tensors untuk keperluan training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"KONVERSI KE PYTORCH TENSORS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Training set tensors\n",
    "print(\"Memproses training set...\")\n",
    "input_ids_train = X_train_tokenized['input_ids']\n",
    "attention_mask_train = X_train_tokenized['attention_mask']\n",
    "token_type_ids_train = X_train_tokenized['token_type_ids']\n",
    "labels_train = torch.tensor(df_train_oversampled['sentiment_encoded'].values.astype(int))\n",
    "print(f\"  âœ“ Labels shape: {labels_train.shape}\")\n",
    "\n",
    "# Validation set tensors\n",
    "print(\"Memproses validation set...\")\n",
    "input_ids_val = X_val_tokenized['input_ids']\n",
    "attention_mask_val = X_val_tokenized['attention_mask']\n",
    "token_type_ids_val = X_val_tokenized['token_type_ids']\n",
    "labels_val = torch.tensor(df_val['sentiment_encoded'].values.astype(int))\n",
    "print(f\"  âœ“ Labels shape: {labels_val.shape}\")\n",
    "\n",
    "# Testing set tensors\n",
    "print(\"Memproses testing set...\")\n",
    "input_ids_test = X_test_tokenized['input_ids']\n",
    "attention_mask_test = X_test_tokenized['attention_mask']\n",
    "token_type_ids_test = X_test_tokenized['token_type_ids']\n",
    "labels_test = torch.tensor(df_test['sentiment_encoded'].values.astype(int))\n",
    "print(f\"  âœ“ Labels shape: {labels_test.shape}\")\n",
    "\n",
    "print(\"\\nâœ“ Semua data berhasil dikonversi ke PyTorch tensors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "## 18. Pembuatan TensorDataset dan DataLoader\n",
    "\n",
    "TensorDataset menggabungkan input tensors dan labels, sedangkan DataLoader menangani batching dan shuffling untuk proses training yang efisien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buat TensorDataset untuk setiap split\n",
    "print(\"=\" * 80)\n",
    "print(\"PEMBUATAN TENSORDATASET\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "train_dataset = TensorDataset(\n",
    "    input_ids_train,\n",
    "    attention_mask_train,\n",
    "    token_type_ids_train,\n",
    "    labels_train\n",
    ")\n",
    "print(f\"Training dataset   : {len(train_dataset)} samples\")\n",
    "\n",
    "val_dataset = TensorDataset(\n",
    "    input_ids_val,\n",
    "    attention_mask_val,\n",
    "    token_type_ids_val,\n",
    "    labels_val\n",
    ")\n",
    "print(f\"Validation dataset : {len(val_dataset)} samples\")\n",
    "\n",
    "test_dataset = TensorDataset(\n",
    "    input_ids_test,\n",
    "    attention_mask_test,\n",
    "    token_type_ids_test,\n",
    "    labels_test\n",
    ")\n",
    "print(f\"Testing dataset    : {len(test_dataset)} samples\")\n",
    "\n",
    "# Buat DataLoader dengan batch size yang optimal\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PEMBUATAN DATALOADER\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Batch size: {BATCH_SIZE}\\n\")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True  # Shuffle untuk training agar model tidak belajar dari urutan data\n",
    ")\n",
    "print(f\"Training DataLoader   : {len(train_dataloader)} batches\")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False  # Tidak perlu shuffle untuk validation\n",
    ")\n",
    "print(f\"Validation DataLoader : {len(val_dataloader)} batches\")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False  # Tidak perlu shuffle untuk testing\n",
    ")\n",
    "print(f\"Testing DataLoader    : {len(test_dataloader)} batches\")\n",
    "\n",
    "print(\"\\nâœ“ TensorDataset dan DataLoader berhasil dibuat.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "## 19. Data Dictionary\n",
    "\n",
    "Data dictionary menjelaskan setiap variabel/kolom dalam dataset yang sudah diproses, termasuk tipe data, deskripsi, dan contoh nilai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membuat data dictionary\n",
    "data_dictionary = {\n",
    "    'Nama Kolom': [\n",
    "        'input',\n",
    "        'output',\n",
    "        'cleaned_input',\n",
    "        'sentiment',\n",
    "        'sentiment_encoded',\n",
    "        'text_length_chars',\n",
    "        'text_length_words'\n",
    "    ],\n",
    "    'Tipe Data': [\n",
    "        'object (string)',\n",
    "        'object (string)',\n",
    "        'object (string)',\n",
    "        'object (string)',\n",
    "        'int64',\n",
    "        'int64',\n",
    "        'int64'\n",
    "    ],\n",
    "    'Deskripsi': [\n",
    "        'Teks input original dari dataset sebelum preprocessing',\n",
    "        'Label output original dengan format prefix (A:, B:, dst) dan kategori sentimen',\n",
    "        'Teks input yang sudah dibersihkan dari URL, HTML tags, karakter khusus, dan whitespace yang tidak perlu',\n",
    "        'Label sentimen kategorikal hasil ekstraksi dan normalisasi (positive, negative, neutral)',\n",
    "        'Label sentimen dalam format numerik untuk keperluan machine learning (0: positive, 1: neutral, 2: negative)',\n",
    "        'Jumlah karakter dalam teks yang sudah dibersihkan',\n",
    "        'Jumlah kata dalam teks yang sudah dibersihkan'\n",
    "    ],\n",
    "    'Contoh Nilai': [\n",
    "        'Check out this amazing product! http://example.com',\n",
    "        'A: very positive',\n",
    "        'Check out this amazing product',\n",
    "        'positive',\n",
    "        '0',\n",
    "        '34',\n",
    "        '5'\n",
    "    ],\n",
    "    'Missing Values': [\n",
    "        '0',\n",
    "        '0',\n",
    "        '0',\n",
    "        '0',\n",
    "        '0',\n",
    "        '0',\n",
    "        '0'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_dictionary = pd.DataFrame(data_dictionary)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DATA DICTIONARY - DATASET PREPROCESSING\")\n",
    "print(\"=\" * 80)\n",
    "display(df_dictionary)\n",
    "\n",
    "# Informasi tambahan tentang encoding\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SENTIMENT ENCODING REFERENCE\")\n",
    "print(\"=\" * 80)\n",
    "for sentiment, code in sentiment_mapping.items():\n",
    "    print(f\"{code} â†’ {sentiment}\")\n",
    "\n",
    "# Informasi tentang transformasi yang dilakukan\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TRANSFORMASI DATA YANG DILAKUKAN\")\n",
    "print(\"=\" * 80)\n",
    "transformations = [\n",
    "    \"1. Cleaning: Menghapus duplikasi data berdasarkan kolom 'input'\",\n",
    "    \"2. Handling Missing Values: Menghapus baris dengan missing values pada kolom 'input' dan 'output'\",\n",
    "    \"3. Text Cleaning: Menghapus URL, HTML tags, karakter khusus, dan normalisasi whitespace\",\n",
    "    \"4. Label Extraction: Ekstraksi label sentimen dari kolom 'output'\",\n",
    "    \"5. Label Normalization: Menggabungkan 'very positive' dengan 'positive' dan 'very negative' dengan 'negative'\",\n",
    "    \"6. Categorical Encoding: Mengubah label sentimen kategorikal menjadi numerik\",\n",
    "    \"7. Data Split: Membagi data menjadi training (70%), validation (15%), testing (15%) dengan stratifikasi\",\n",
    "    \"8. Oversampling: Menyeimbangkan kelas pada training set menggunakan random oversampling\",\n",
    "    \"9. Tokenization: Mengkonversi teks menjadi token numerik menggunakan BERT tokenizer\",\n",
    "    \"10. Tensor Conversion: Mengkonversi data menjadi PyTorch tensors untuk model training\"\n",
    "]\n",
    "\n",
    "for transformation in transformations:\n",
    "    print(f\"  {transformation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "## 20. Summary dan Hasil Akhir Preprocessing\n",
    "\n",
    "Ringkasan lengkap dari semua proses preprocessing yang telah dilakukan, termasuk statistik dataset dan output yang dihasilkan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"LAPORAN AKHIR DATA WRANGLING DAN PREPROCESSING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nðŸ“Š STATISTIK DATASET\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Dataset original          : {len(df):,} baris\")\n",
    "print(f\"Baris duplikat dihapus    : {removed_duplicates:,} baris\")\n",
    "print(f\"Dataset setelah cleaning  : {len(df_clean):,} baris\")\n",
    "print(f\"Missing values ditangani  : Ya (drop baris dengan missing values pada kolom penting)\")\n",
    "\n",
    "print(\"\\nðŸ“‚ DATA SPLIT\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Training set (original)   : {len(df_train):,} baris (70%)\")\n",
    "print(f\"Training set (oversampled): {len(df_train_oversampled):,} baris\")\n",
    "print(f\"Validation set            : {len(df_val):,} baris (15%)\")\n",
    "print(f\"Testing set               : {len(df_test):,} baris (15%)\")\n",
    "\n",
    "print(\"\\nðŸ”„ TRANSFORMASI\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"Text cleaning             : âœ“ Selesai\")\n",
    "print(f\"Label encoding            : âœ“ Selesai (3 kelas)\")\n",
    "print(f\"Oversampling              : âœ“ Selesai (balance ratio 1:1:1)\")\n",
    "print(f\"Tokenization              : âœ“ Selesai (BERT, max_length={MAX_LENGTH})\")\n",
    "print(f\"Tensor conversion         : âœ“ Selesai (PyTorch tensors)\")\n",
    "\n",
    "print(\"\\nðŸ“ˆ DISTRIBUSI SENTIMEN (Training Set Oversampled)\")\n",
    "print(\"-\" * 80)\n",
    "for sentiment in ['negative', 'neutral', 'positive']:\n",
    "    count = (df_train_oversampled['sentiment'] == sentiment).sum()\n",
    "    percentage = count / len(df_train_oversampled) * 100\n",
    "    print(f\"{sentiment.capitalize():10s}: {count:6,} samples ({percentage:5.1f}%)\")\n",
    "\n",
    "print(\"\\nâœ… OUTPUT YANG DIHASILKAN\")\n",
    "print(\"-\" * 80)\n",
    "outputs = [\n",
    "    \"1. Dataset bersih (df_clean): Dataset yang sudah melalui cleaning, handling missing values, dan transformasi\",\n",
    "    \"2. Training set (df_train_oversampled): Data training yang sudah di-balance dengan oversampling\",\n",
    "    \"3. Validation set (df_val): Data untuk validasi model selama training\",\n",
    "    \"4. Testing set (df_test): Data untuk evaluasi performa final model\",\n",
    "    \"5. PyTorch DataLoaders: train_dataloader, val_dataloader, test_dataloader siap untuk model training\",\n",
    "    \"6. Data Dictionary: Dokumentasi lengkap setiap variabel dalam dataset\",\n",
    "    \"7. Laporan Preprocessing: Dokumentasi semua tahapan dan hasil preprocessing\"\n",
    "]\n",
    "\n",
    "for output in outputs:\n",
    "    print(f\"  {output}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PREPROCESSING SELESAI - DATASET SIAP UNTUK MODEL TRAINING\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "## 21. Ekspor Dataset Bersih (Opsional)\n",
    "\n",
    "Menyimpan dataset yang sudah dibersihkan untuk keperluan dokumentasi atau analisis lebih lanjut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simpan dataset bersih ke CSV (opsional)\n",
    "output_filename = 'sentiment_dataset_cleaned.csv'\n",
    "\n",
    "# Pilih kolom yang relevan untuk disimpan\n",
    "columns_to_save = [\n",
    "    'input',\n",
    "    'output',\n",
    "    'cleaned_input',\n",
    "    'sentiment',\n",
    "    'sentiment_encoded',\n",
    "    'text_length_chars',\n",
    "    'text_length_words'\n",
    "]\n",
    "\n",
    "df_clean[columns_to_save].to_csv(output_filename, index=False)\n",
    "\n",
    "print(f\"âœ“ Dataset bersih berhasil disimpan ke: {output_filename}\")\n",
    "print(f\"  Total baris: {len(df_clean):,}\")\n",
    "print(f\"  Total kolom: {len(columns_to_save)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Kesimpulan\n",
    "\n",
    "Proses data wrangling dan preprocessing telah berhasil diselesaikan dengan tahapan sebagai berikut:\n",
    "\n",
    "### 1. **Data Cleaning**\n",
    "   - Menghapus duplikasi data untuk menghindari bias\n",
    "   - Menangani missing values dengan strategi drop pada kolom penting\n",
    "   - Membersihkan teks dari noise (URL, HTML tags, karakter khusus)\n",
    "   - Normalisasi whitespace untuk konsistensi\n",
    "\n",
    "### 2. **Penanganan Missing Values**\n",
    "   - Identifikasi missing values pada semua kolom\n",
    "   - Penghapusan baris dengan missing values pada kolom 'input' dan 'output'\n",
    "   - Verifikasi tidak ada missing values setelah cleaning\n",
    "\n",
    "### 3. **Transformasi Data**\n",
    "   - **Encoding kategorikal**: Label sentimen diubah menjadi numerik (positive: 0, neutral: 1, negative: 2)\n",
    "   - **Normalisasi label**: Menggabungkan kategori sentimen yang mirip (very positive â†’ positive, very negative â†’ negative)\n",
    "   - **Oversampling**: Menyeimbangkan distribusi kelas pada training set menggunakan random oversampling\n",
    "   - **Tokenization**: Mengkonversi teks menjadi format yang sesuai untuk model BERT\n",
    "\n",
    "### 4. **Data Dictionary**\n",
    "   - Dokumentasi lengkap untuk setiap variabel dalam dataset\n",
    "   - Penjelasan tipe data, deskripsi, dan contoh nilai\n",
    "   - Referensi encoding untuk label sentimen\n",
    "\n",
    "### Output Akhir:\n",
    "- âœ… Dataset bersih dan siap digunakan\n",
    "- âœ… Data split dengan proporsi optimal (70-15-15)\n",
    "- âœ… Training set yang balanced untuk menghindari bias model\n",
    "- âœ… PyTorch DataLoaders siap untuk training model BERT\n",
    "- âœ… Dokumentasi lengkap proses preprocessing\n",
    "\n",
    "Dataset sekarang siap untuk tahap selanjutnya: **Model Training dan Evaluation**.\n",
    "\n",
    "---\n",
    "\n",
    "**Catatan**: Semua transformasi dilakukan dengan mempertahankan integritas data dan mencegah data leakage antara training, validation, dan testing set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
