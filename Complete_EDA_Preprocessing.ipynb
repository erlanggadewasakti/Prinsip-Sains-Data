{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "---\n",
    "# PART 1: SETUP & DATA LOADING\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## 1.1 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Text processing\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup\n",
    "\n",
    "# Deep learning\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# Utilities\n",
    "import time\n",
    "import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "sns.set_style(\"whitegrid\")\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úì All libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## 1.2 Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from GitHub release\n",
    "try:\n",
    "    df = pd.read_csv('https://github.com/erlanggadewasakti/Prinsip-Sains-Data/releases/download/prod/sa-psd-dataset.csv')\n",
    "    print(f\"‚úì Dataset loaded successfully\")\n",
    "    print(f\"  Rows: {len(df):,}\")\n",
    "    print(f\"  Columns: {len(df.columns)}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚úó Error loading dataset: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## 1.3 Initial Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nüìä First 5 Rows:\")\n",
    "display(df.head())\n",
    "\n",
    "print(\"\\nüìã Dataset Info:\")\n",
    "df.info()\n",
    "\n",
    "print(\"\\nüìà Statistical Summary:\")\n",
    "display(df.describe(include='all'))\n",
    "\n",
    "print(\"\\nüîç Missing Values:\")\n",
    "missing_info = pd.DataFrame({\n",
    "    'Column': df.columns,\n",
    "    'Missing Count': df.isnull().sum().values,\n",
    "    'Percentage': (df.isnull().sum().values / len(df) * 100).round(2)\n",
    "})\n",
    "display(missing_info[missing_info['Missing Count'] > 0])\n",
    "\n",
    "if df.isnull().sum().sum() == 0:\n",
    "    print(\"‚úì No missing values detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "---\n",
    "# PART 2: EXPLORATORY DATA ANALYSIS (EDA)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## 2.1 Extract & Normalize Sentiment Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract sentiment from output column\n",
    "df['sentiment'] = df['output'].str.replace(r'^[A-E]:\\s*', '', regex=True)\n",
    "\n",
    "print(\"Sentiment Distribution (Before Normalization):\")\n",
    "print(df['sentiment'].value_counts())\n",
    "\n",
    "# Normalize: merge \"very positive/negative\" with \"positive/negative\"\n",
    "sentiment_mapping_initial = {\n",
    "    'very positive': 'positive',\n",
    "    'very negative': 'negative',\n",
    "    'positive': 'positive',\n",
    "    'negative': 'negative',\n",
    "    'neutral': 'neutral'\n",
    "}\n",
    "\n",
    "df['sentiment'] = df['sentiment'].map(sentiment_mapping_initial)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Sentiment Distribution (After Normalization):\")\n",
    "print(df['sentiment'].value_counts())\n",
    "print(\"\\nProportions:\")\n",
    "print(df['sentiment'].value_counts(normalize=True).round(4))\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nSample Data:\")\n",
    "display(df[['input', 'output', 'sentiment']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## 2.2 Sentiment Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_colors = {'positive': '#2ecc71', 'negative': '#e74c3c', 'neutral': '#3498db'}\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Count plot\n",
    "sns.countplot(x='sentiment', data=df, hue='sentiment', palette=sentiment_colors, legend=False, ax=axes[0])\n",
    "axes[0].set_title('Sentiment Distribution (Count)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Sentiment', fontsize=12)\n",
    "axes[0].set_ylabel('Count', fontsize=12)\n",
    "for container in axes[0].containers:\n",
    "    axes[0].bar_label(container, fmt='%d')\n",
    "\n",
    "# Pie chart\n",
    "sentiment_counts = df['sentiment'].value_counts()\n",
    "colors = [sentiment_colors[label] for label in sentiment_counts.index]\n",
    "axes[1].pie(sentiment_counts.values, labels=sentiment_counts.index, autopct='%1.1f%%',\n",
    "            colors=colors, startangle=90, textprops={'fontsize': 12})\n",
    "axes[1].set_title('Sentiment Distribution (Proportion)', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Imbalance analysis\n",
    "majority_class = sentiment_counts.idxmax()\n",
    "minority_class = sentiment_counts.idxmin()\n",
    "imbalance_ratio = sentiment_counts.max() / sentiment_counts.min()\n",
    "\n",
    "print(f\"\\n{'=' * 50}\")\n",
    "print(\"CLASS BALANCE ANALYSIS\")\n",
    "print(f\"{'=' * 50}\")\n",
    "print(f\"Majority Class: {majority_class} ({sentiment_counts.max():,} samples)\")\n",
    "print(f\"Minority Class: {minority_class} ({sentiment_counts.min():,} samples)\")\n",
    "print(f\"Imbalance Ratio: {imbalance_ratio:.2f}:1\")\n",
    "\n",
    "if imbalance_ratio > 3:\n",
    "    print(\"\\n‚ö†Ô∏è  Dataset is IMBALANCED - Oversampling recommended!\")\n",
    "else:\n",
    "    print(\"\\n‚úì Dataset is relatively balanced\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## 2.3 Basic Text Cleaning for EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic cleaning: lowercase\n",
    "df['cleaned_input'] = df['input'].str.lower()\n",
    "\n",
    "print(\"Sample Text Comparison (Original vs Cleaned):\")\n",
    "for idx in range(3):\n",
    "    print(f\"\\n[{idx+1}] Original: {df['input'].iloc[idx][:80]}...\")\n",
    "    print(f\"    Cleaned:  {df['cleaned_input'].iloc[idx][:80]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## 2.4 Text Statistics Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate text statistics\n",
    "df['char_length'] = df['cleaned_input'].str.len()\n",
    "df['word_count'] = df['cleaned_input'].str.split().str.len()\n",
    "df['avg_word_length'] = df['char_length'] / df['word_count']\n",
    "df['avg_word_length'] = df['avg_word_length'].fillna(0)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TEXT STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nüìè Character Length:\")\n",
    "print(df['char_length'].describe())\n",
    "\n",
    "print(\"\\nüìù Word Count:\")\n",
    "print(df['word_count'].describe())\n",
    "\n",
    "print(\"\\nüìä Average Word Length:\")\n",
    "print(df['avg_word_length'].describe())\n",
    "\n",
    "# Display sample\n",
    "display(df[['cleaned_input', 'char_length', 'word_count', 'avg_word_length']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## 2.5 Text Statistics Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histograms by sentiment\n",
    "text_stats = ['char_length', 'word_count', 'avg_word_length']\n",
    "stat_titles = ['Character Length', 'Word Count', 'Average Word Length']\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, (stat, title) in enumerate(zip(text_stats, stat_titles)):\n",
    "    sns.histplot(data=df, x=stat, hue='sentiment', kde=True, multiple='stack',\n",
    "                 palette=sentiment_colors, ax=axes[idx], alpha=0.7)\n",
    "    axes[idx].set_title(f'Distribution of {title}', fontweight='bold')\n",
    "    axes[idx].set_xlabel(title)\n",
    "    axes[idx].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Boxplots by sentiment\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, (stat, title) in enumerate(zip(text_stats, stat_titles)):\n",
    "    sns.boxplot(data=df, x='sentiment', y=stat, palette=sentiment_colors, ax=axes[idx])\n",
    "    axes[idx].set_title(f'{title} by Sentiment', fontweight='bold')\n",
    "    axes[idx].set_xlabel('Sentiment')\n",
    "    axes[idx].set_ylabel(title)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## 2.6 N-Gram Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download stopwords\n",
    "try:\n",
    "    stopwords_english = stopwords.words('english')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "    stopwords_english = stopwords.words('english')\n",
    "\n",
    "print(\"‚úì Stopwords loaded\")\n",
    "\n",
    "# N-gram analysis per sentiment\n",
    "ngram_ranges = [(1, 1), (2, 2), (3, 3)]\n",
    "ngram_names = ['Unigrams', 'Bigrams', 'Trigrams']\n",
    "\n",
    "for sentiment in df['sentiment'].unique():\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"ANALYZING SENTIMENT: {sentiment.upper()}\")\n",
    "    print(f\"{'=' * 60}\")\n",
    "\n",
    "    sentiment_df = df[df['sentiment'] == sentiment]\n",
    "    cleaned_text = sentiment_df['cleaned_input'].dropna()\n",
    "\n",
    "    if cleaned_text.empty:\n",
    "        print(f\"‚ö†Ô∏è  No text available for sentiment: {sentiment}\")\n",
    "        continue\n",
    "\n",
    "    current_color = sentiment_colors.get(sentiment, 'gray')\n",
    "\n",
    "    for n_range, n_name in zip(ngram_ranges, ngram_names):\n",
    "        vectorizer = CountVectorizer(ngram_range=n_range, stop_words=stopwords_english)\n",
    "\n",
    "        try:\n",
    "            X = vectorizer.fit_transform(cleaned_text)\n",
    "            sum_words = X.sum(axis=0)\n",
    "            words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n",
    "            words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
    "            top_ngrams = words_freq[:20]\n",
    "\n",
    "            if not top_ngrams:\n",
    "                continue\n",
    "\n",
    "            top_ngrams_df = pd.DataFrame(top_ngrams, columns=['ngram', 'count'])\n",
    "\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            sns.barplot(x='count', y='ngram', data=top_ngrams_df, color=current_color, edgecolor='black')\n",
    "            plt.title(f'Top 20 {n_name} - {sentiment.capitalize()} Sentiment', fontsize=14, fontweight='bold')\n",
    "            plt.xlabel('Frequency', fontsize=12)\n",
    "            plt.ylabel(n_name, fontsize=12)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        except ValueError as e:\n",
    "            print(f\"  ‚ö†Ô∏è  Could not analyze {n_name}: {e}\")\n",
    "\n",
    "print(\"\\n‚úì N-gram analysis complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## 2.7 Word Cloud Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word cloud for all text\n",
    "all_text = ' '.join(df['cleaned_input'].dropna())\n",
    "wordcloud_all = WordCloud(width=1200, height=600, background_color='white',\n",
    "                          stopwords=stopwords_english, colormap='viridis').generate(all_text)\n",
    "\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.imshow(wordcloud_all, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud - All Sentiments (Stopwords Removed)', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Get overall most common words\n",
    "all_words = all_text.split()\n",
    "all_words_filtered = [word for word in all_words if word not in stopwords_english]\n",
    "overall_word_counts = Counter(all_words_filtered)\n",
    "most_common_overall = set([word for word, count in overall_word_counts.most_common(50)])\n",
    "\n",
    "# Word clouds per sentiment (excluding overall common words)\n",
    "for sentiment in df['sentiment'].unique():\n",
    "    sentiment_text = ' '.join(df[df['sentiment'] == sentiment]['cleaned_input'].dropna())\n",
    "\n",
    "    if sentiment_text:\n",
    "        sentiment_words = sentiment_text.split()\n",
    "        sentiment_words_filtered = [word for word in sentiment_words\n",
    "                                   if word not in most_common_overall and word not in stopwords_english]\n",
    "        filtered_text = ' '.join(sentiment_words_filtered)\n",
    "\n",
    "        if filtered_text:\n",
    "            wordcloud = WordCloud(width=1200, height=600, background_color='white',\n",
    "                                 colormap='RdYlGn' if sentiment == 'positive' else\n",
    "                                         ('Blues' if sentiment == 'neutral' else 'Reds')).generate(filtered_text)\n",
    "\n",
    "            plt.figure(figsize=(14, 7))\n",
    "            plt.imshow(wordcloud, interpolation='bilinear')\n",
    "            plt.axis('off')\n",
    "            plt.title(f'Word Cloud - {sentiment.capitalize()} Sentiment (Unique Words)',\n",
    "                     fontsize=16, fontweight='bold', pad=20)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  No unique words for {sentiment} after filtering\")\n",
    "\n",
    "print(\"\\n‚úì Word cloud analysis complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "---\n",
    "# PART 3: DATA PREPROCESSING & CLEANING\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## 3.1 Handle Missing Values & Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"DATA QUALITY CHECKS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check missing values\n",
    "print(\"\\n1Ô∏è‚É£  Missing Values:\")\n",
    "missing_critical = df[['input', 'output']].isnull().sum()\n",
    "print(missing_critical)\n",
    "\n",
    "if missing_critical.sum() > 0:\n",
    "    initial_rows = len(df)\n",
    "    df = df.dropna(subset=['input', 'output'])\n",
    "    print(f\"   Dropped {initial_rows - len(df)} rows with missing values\")\n",
    "else:\n",
    "    print(\"   ‚úì No missing values in critical columns\")\n",
    "\n",
    "# Check duplicates\n",
    "print(\"\\n2Ô∏è‚É£  Duplicate Analysis:\")\n",
    "duplicates_all = df.duplicated().sum()\n",
    "duplicates_input = df.duplicated(subset=['input']).sum()\n",
    "\n",
    "print(f\"   Full duplicates: {duplicates_all}\")\n",
    "print(f\"   Input duplicates: {duplicates_input}\")\n",
    "\n",
    "if duplicates_input > 0:\n",
    "    print(f\"\\n   Sample duplicate texts:\")\n",
    "    duplicate_samples = df[df.duplicated(subset=['input'], keep=False)].sort_values('input').head(6)\n",
    "    display(duplicate_samples[['input', 'sentiment']])\n",
    "\n",
    "    initial_rows = len(df)\n",
    "    df = df.drop_duplicates(subset=['input'], keep='first')\n",
    "    df = df.reset_index(drop=True)\n",
    "    print(f\"\\n   ‚úì Removed {initial_rows - len(df)} duplicate rows\")\n",
    "else:\n",
    "    print(\"   ‚úì No duplicates found\")\n",
    "\n",
    "print(f\"\\nüìä Final dataset size: {len(df):,} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## 3.2 Advanced Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_text_cleaning(text):\n",
    "    \"\"\"\n",
    "    Advanced text cleaning for LLM compatibility:\n",
    "    - Remove URLs\n",
    "    - Remove HTML tags\n",
    "    - Remove special characters\n",
    "    - Normalize whitespace\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return str(text) if text is not None else \"\"\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "\n",
    "    # Remove special characters (keep letters, numbers, spaces)\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "\n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "# Apply advanced cleaning\n",
    "df['cleaned_input'] = df['input'].apply(advanced_text_cleaning)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TEXT CLEANING RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nComparison (Original vs Cleaned):\")\n",
    "for idx in range(3):\n",
    "    print(f\"\\n[{idx+1}] Original:\")\n",
    "    print(f\"    {df['input'].iloc[idx][:100]}...\")\n",
    "    print(f\"    Cleaned:\")\n",
    "    print(f\"    {df['cleaned_input'].iloc[idx][:100]}...\")\n",
    "\n",
    "# Check for empty strings after cleaning\n",
    "empty_count = (df['cleaned_input'].str.len() == 0).sum()\n",
    "print(f\"\\nüìä Empty strings after cleaning: {empty_count}\")\n",
    "\n",
    "if empty_count > 0:\n",
    "    df = df[df['cleaned_input'].str.len() > 0].reset_index(drop=True)\n",
    "    print(f\"   Removed {empty_count} empty rows\")\n",
    "\n",
    "print(f\"\\n‚úì Text cleaning complete - {len(df):,} rows remaining\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "## 3.3 Sentiment Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode sentiment labels to numerical values\n",
    "sentiment_encoding = {\n",
    "    'positive': 0,\n",
    "    'neutral': 1,\n",
    "    'negative': 2\n",
    "}\n",
    "\n",
    "df['sentiment_encoded'] = df['sentiment'].map(sentiment_encoding)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"LABEL ENCODING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nSentiment Mapping:\")\n",
    "for sentiment, code in sentiment_encoding.items():\n",
    "    count = (df['sentiment_encoded'] == code).sum()\n",
    "    percentage = count / len(df) * 100\n",
    "    print(f\"  {sentiment.capitalize():12s} -> {code}  ({count:6,} samples, {percentage:5.2f}%)\")\n",
    "\n",
    "# Verify encoding\n",
    "assert df['sentiment_encoded'].isnull().sum() == 0, \"‚ùå Encoding failed - null values found!\"\n",
    "print(\"\\n‚úì Label encoding successful\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nSample Encoded Data:\")\n",
    "display(df[['cleaned_input', 'sentiment', 'sentiment_encoded']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "## 3.4 Text Length Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate text length statistics\n",
    "df['text_length_chars'] = df['cleaned_input'].str.len()\n",
    "df['text_length_words'] = df['cleaned_input'].str.split().str.len()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TEXT LENGTH STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nüìè Character Length:\")\n",
    "print(df['text_length_chars'].describe())\n",
    "\n",
    "print(\"\\nüìù Word Count:\")\n",
    "print(df['text_length_words'].describe())\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Character length distribution\n",
    "axes[0].hist(df['text_length_chars'], bins=50, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "axes[0].axvline(df['text_length_chars'].median(), color='red', linestyle='--', linewidth=2,\n",
    "                label=f\"Median: {df['text_length_chars'].median():.0f}\")\n",
    "axes[0].set_xlabel('Number of Characters', fontsize=12)\n",
    "axes[0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0].set_title('Distribution of Text Length (Characters)', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Word count distribution\n",
    "axes[1].hist(df['text_length_words'], bins=50, color='lightcoral', edgecolor='black', alpha=0.7)\n",
    "axes[1].axvline(df['text_length_words'].median(), color='blue', linestyle='--', linewidth=2,\n",
    "                label=f\"Median: {df['text_length_words'].median():.0f}\")\n",
    "axes[1].set_xlabel('Number of Words', fontsize=12)\n",
    "axes[1].set_ylabel('Frequency', fontsize=12)\n",
    "axes[1].set_title('Distribution of Text Length (Words)', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "---\n",
    "# PART 4: DATA SPLITTING & BALANCING\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "## 4.1 Train/Validation/Test Split (70/15/15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DATA SPLITTING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Split: Training (70%) and Temporary (30%)\n",
    "df_train, df_temp = train_test_split(\n",
    "    df, test_size=0.3, random_state=RANDOM_STATE,\n",
    "    stratify=df['sentiment_encoded']\n",
    ")\n",
    "\n",
    "# Split Temporary: Validation (15%) and Test (15%)\n",
    "df_val, df_test = train_test_split(\n",
    "    df_temp, test_size=0.5, random_state=RANDOM_STATE,\n",
    "    stratify=df_temp['sentiment_encoded']\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Dataset Split:\")\n",
    "print(f\"   Training   : {len(df_train):6,} samples ({len(df_train)/len(df)*100:5.1f}%)\")\n",
    "print(f\"   Validation : {len(df_val):6,} samples ({len(df_val)/len(df)*100:5.1f}%)\")\n",
    "print(f\"   Test       : {len(df_test):6,} samples ({len(df_test)/len(df)*100:5.1f}%)\")\n",
    "print(f\"   Total      : {len(df):6,} samples\")\n",
    "\n",
    "# Verify stratification\n",
    "print(f\"\\nüîç Sentiment Distribution Across Splits:\")\n",
    "print(\"\\nTraining:\")\n",
    "print(df_train['sentiment'].value_counts(normalize=True).sort_index())\n",
    "print(\"\\nValidation:\")\n",
    "print(df_val['sentiment'].value_counts(normalize=True).sort_index())\n",
    "print(\"\\nTest:\")\n",
    "print(df_test['sentiment'].value_counts(normalize=True).sort_index())\n",
    "\n",
    "print(\"\\n‚úì Data split successful with stratification maintained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "## 4.2 Oversampling Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"OVERSAMPLING TRAINING SET\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Identify target count (majority class)\n",
    "majority_count = df_train['sentiment'].value_counts().max()\n",
    "print(f\"\\nTarget samples per class: {majority_count:,}\")\n",
    "\n",
    "print(\"\\nBefore Oversampling:\")\n",
    "print(df_train['sentiment'].value_counts().sort_index())\n",
    "\n",
    "# Oversample minority classes\n",
    "df_train_oversampled = pd.DataFrame()\n",
    "\n",
    "for sentiment_label in df_train['sentiment'].unique():\n",
    "    sentiment_df = df_train[df_train['sentiment'] == sentiment_label]\n",
    "    current_count = len(sentiment_df)\n",
    "\n",
    "    if current_count < majority_count:\n",
    "        # Oversample with replacement\n",
    "        oversampled_df = sentiment_df.sample(n=majority_count, replace=True, random_state=RANDOM_STATE)\n",
    "        df_train_oversampled = pd.concat([df_train_oversampled, oversampled_df], ignore_index=True)\n",
    "        print(f\"  {sentiment_label.capitalize():12s}: {current_count:6,} -> {len(oversampled_df):6,} (+{len(oversampled_df)-current_count:,})\")\n",
    "    else:\n",
    "        df_train_oversampled = pd.concat([df_train_oversampled, sentiment_df], ignore_index=True)\n",
    "        print(f\"  {sentiment_label.capitalize():12s}: {current_count:6,} (unchanged)\")\n",
    "\n",
    "# Shuffle\n",
    "df_train_oversampled = df_train_oversampled.sample(frac=1, random_state=RANDOM_STATE).reset_index(drop=True)\n",
    "\n",
    "print(\"\\nAfter Oversampling:\")\n",
    "print(df_train_oversampled['sentiment'].value_counts().sort_index())\n",
    "\n",
    "print(f\"\\nüìä Training set size: {len(df_train):,} -> {len(df_train_oversampled):,} (+{len(df_train_oversampled)-len(df_train):,})\")\n",
    "print(\"‚úì Oversampling complete - Classes balanced\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "## 4.3 Visualize Oversampling Effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Before oversampling\n",
    "df_train['sentiment'].value_counts().sort_index().plot(\n",
    "    kind='bar', ax=axes[0], color=['#e74c3c', '#3498db', '#2ecc71'],\n",
    "    edgecolor='black', alpha=0.8\n",
    ")\n",
    "axes[0].set_title('Training Set - BEFORE Oversampling', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Sentiment', fontsize=12)\n",
    "axes[0].set_ylabel('Count', fontsize=12)\n",
    "axes[0].set_xticklabels(['Negative', 'Neutral', 'Positive'], rotation=0)\n",
    "for container in axes[0].containers:\n",
    "    axes[0].bar_label(container, fmt='%d', fontsize=10)\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# After oversampling\n",
    "df_train_oversampled['sentiment'].value_counts().sort_index().plot(\n",
    "    kind='bar', ax=axes[1], color=['#e74c3c', '#3498db', '#2ecc71'],\n",
    "    edgecolor='black', alpha=0.8\n",
    ")\n",
    "axes[1].set_title('Training Set - AFTER Oversampling', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Sentiment', fontsize=12)\n",
    "axes[1].set_ylabel('Count', fontsize=12)\n",
    "axes[1].set_xticklabels(['Negative', 'Neutral', 'Positive'], rotation=0)\n",
    "for container in axes[1].containers:\n",
    "    axes[1].bar_label(container, fmt='%d', fontsize=10)\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "---\n",
    "# PART 5: TOKENIZATION & MODEL PREPARATION\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "## 5.1 Load BERT Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BERT tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BERT TOKENIZER\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nModel: bert-base-uncased\")\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size:,}\")\n",
    "print(f\"Model max length: {tokenizer.model_max_length:,}\")\n",
    "print(\"\\n‚úì Tokenizer loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "## 5.2 Analyze Token Length Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"TOKEN LENGTH ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calculate token lengths for training data\n",
    "print(\"\\nTokenizing training data to analyze length distribution...\")\n",
    "token_lengths = [\n",
    "    len(tokenizer.encode(str(text), add_special_tokens=True, truncation=True, max_length=512))\n",
    "    for text in df_train_oversampled['cleaned_input']\n",
    "]\n",
    "\n",
    "# Statistics\n",
    "print(\"\\nüìä Token Length Statistics:\")\n",
    "print(f\"   Min        : {np.min(token_lengths)}\")\n",
    "print(f\"   Max        : {np.max(token_lengths)}\")\n",
    "print(f\"   Mean       : {np.mean(token_lengths):.2f}\")\n",
    "print(f\"   Median     : {np.median(token_lengths):.0f}\")\n",
    "print(f\"   P95        : {np.percentile(token_lengths, 95):.0f}\")\n",
    "print(f\"   P99        : {np.percentile(token_lengths, 99):.0f}\")\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.histplot(token_lengths, bins=50, kde=True, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "plt.axvline(np.mean(token_lengths), color='red', linestyle='--', linewidth=2,\n",
    "            label=f'Mean: {np.mean(token_lengths):.0f}')\n",
    "plt.axvline(np.percentile(token_lengths, 95), color='orange', linestyle='--', linewidth=2,\n",
    "            label=f'P95: {np.percentile(token_lengths, 95):.0f}')\n",
    "plt.xlabel('Number of Tokens', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.title('Token Length Distribution (Training Set)', fontsize=14, fontweight='bold')\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Determine optimal MAX_LENGTH\n",
    "suggested_max = int(np.percentile(token_lengths, 95))\n",
    "MAX_LENGTH = min(suggested_max, 512)\n",
    "\n",
    "# Use practical value if too large\n",
    "if MAX_LENGTH > 400:\n",
    "    MAX_LENGTH = 330\n",
    "\n",
    "print(f\"\\nüéØ Optimal MAX_LENGTH Selection:\")\n",
    "print(f\"   Suggested (P95): {suggested_max}\")\n",
    "print(f\"   Chosen          : {MAX_LENGTH}\")\n",
    "coverage = (np.array(token_lengths) <= MAX_LENGTH).sum() / len(token_lengths) * 100\n",
    "print(f\"   Coverage        : {coverage:.2f}%\")\n",
    "print(f\"\\n‚úì MAX_LENGTH = {MAX_LENGTH} will cover {coverage:.1f}% of the data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "## 5.3 Tokenize All Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(texts, tokenizer, max_length):\n",
    "    \"\"\"Tokenize text data with padding and truncation\"\"\"\n",
    "    return tokenizer(\n",
    "        list(texts),\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TOKENIZATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nTokenizing datasets with MAX_LENGTH = {MAX_LENGTH}...\")\n",
    "\n",
    "# Tokenize all datasets\n",
    "X_train_tokenized = tokenize_data(df_train_oversampled['cleaned_input'], tokenizer, MAX_LENGTH)\n",
    "X_val_tokenized = tokenize_data(df_val['cleaned_input'], tokenizer, MAX_LENGTH)\n",
    "X_test_tokenized = tokenize_data(df_test['cleaned_input'], tokenizer, MAX_LENGTH)\n",
    "\n",
    "print(f\"\\n‚úì Tokenization complete\")\n",
    "print(f\"\\nüìä Tokenized Shapes (input_ids):\")\n",
    "print(f\"   Training   : {X_train_tokenized['input_ids'].shape}\")\n",
    "print(f\"   Validation : {X_val_tokenized['input_ids'].shape}\")\n",
    "print(f\"   Test       : {X_test_tokenized['input_ids'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "## 5.4 Create PyTorch Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PYTORCH DATASET CREATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "print(\"\\nConverting tokenized data to PyTorch tensors...\")\n",
    "\n",
    "# Training data\n",
    "input_ids_train = X_train_tokenized['input_ids']\n",
    "attention_mask_train = X_train_tokenized['attention_mask']\n",
    "token_type_ids_train = X_train_tokenized['token_type_ids']\n",
    "labels_train = torch.tensor(df_train_oversampled['sentiment_encoded'].values.astype(int))\n",
    "\n",
    "# Validation data\n",
    "input_ids_val = X_val_tokenized['input_ids']\n",
    "attention_mask_val = X_val_tokenized['attention_mask']\n",
    "token_type_ids_val = X_val_tokenized['token_type_ids']\n",
    "labels_val = torch.tensor(df_val['sentiment_encoded'].values.astype(int))\n",
    "\n",
    "# Test data\n",
    "input_ids_test = X_test_tokenized['input_ids']\n",
    "attention_mask_test = X_test_tokenized['attention_mask']\n",
    "token_type_ids_test = X_test_tokenized['token_type_ids']\n",
    "labels_test = torch.tensor(df_test['sentiment_encoded'].values.astype(int))\n",
    "\n",
    "print(\"‚úì Tensors created\")\n",
    "\n",
    "# Create TensorDatasets\n",
    "print(\"\\nCreating TensorDatasets...\")\n",
    "train_dataset = TensorDataset(input_ids_train, attention_mask_train, token_type_ids_train, labels_train)\n",
    "val_dataset = TensorDataset(input_ids_val, attention_mask_val, token_type_ids_val, labels_val)\n",
    "test_dataset = TensorDataset(input_ids_test, attention_mask_test, token_type_ids_test, labels_test)\n",
    "\n",
    "print(\"‚úì TensorDatasets created\")\n",
    "print(f\"\\nüìä Dataset Sizes:\")\n",
    "print(f\"   Training   : {len(train_dataset):,} samples\")\n",
    "print(f\"   Validation : {len(val_dataset):,} samples\")\n",
    "print(f\"   Test       : {len(test_dataset):,} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "## 5.5 Create DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DATALOADER CREATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(f\"\\nBatch size: {BATCH_SIZE}\")\n",
    "print(f\"\\nüìä DataLoader Batches:\")\n",
    "print(f\"   Training   : {len(train_dataloader)} batches\")\n",
    "print(f\"   Validation : {len(val_dataloader)} batches\")\n",
    "print(f\"   Test       : {len(test_dataloader)} batches\")\n",
    "\n",
    "print(\"\\n‚úì DataLoaders ready for model training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "---\n",
    "# FINAL SUMMARY\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "## Pipeline Summary & Data Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"COMPLETE PREPROCESSING PIPELINE SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nüìä DATASET STATISTICS:\")\n",
    "print(f\"   Original dataset        : {len(df):,} samples\")\n",
    "print(f\"   After cleaning          : {len(df):,} samples\")\n",
    "print(f\"   Training (original)     : {len(df_train):,} samples\")\n",
    "print(f\"   Training (oversampled)  : {len(df_train_oversampled):,} samples\")\n",
    "print(f\"   Validation              : {len(df_val):,} samples\")\n",
    "print(f\"   Test                    : {len(df_test):,} samples\")\n",
    "\n",
    "print(\"\\nüîÑ TRANSFORMATIONS APPLIED:\")\n",
    "print(\"   ‚úì Missing value removal\")\n",
    "print(\"   ‚úì Duplicate removal\")\n",
    "print(\"   ‚úì Advanced text cleaning (URLs, HTML, special chars)\")\n",
    "print(\"   ‚úì Sentiment label normalization\")\n",
    "print(\"   ‚úì Label encoding (3 classes: positive=0, neutral=1, negative=2)\")\n",
    "print(\"   ‚úì Stratified train/val/test split (70/15/15)\")\n",
    "print(\"   ‚úì Oversampling on training set (balanced 1:1:1 ratio)\")\n",
    "print(f\"   ‚úì BERT tokenization (max_length={MAX_LENGTH})\")\n",
    "print(\"   ‚úì PyTorch DataLoader creation\")\n",
    "\n",
    "print(\"\\nüìà SENTIMENT DISTRIBUTION (Training - Oversampled):\")\n",
    "train_dist = df_train_oversampled['sentiment'].value_counts().sort_index()\n",
    "for sentiment in ['negative', 'neutral', 'positive']:\n",
    "    count = train_dist[sentiment]\n",
    "    pct = count / len(df_train_oversampled) * 100\n",
    "    print(f\"   {sentiment.capitalize():12s}: {count:6,} samples ({pct:5.1f}%)\")\n",
    "\n",
    "print(\"\\nüéØ MODEL CONFIGURATION:\")\n",
    "print(f\"   Tokenizer          : bert-base-uncased\")\n",
    "print(f\"   Max sequence length: {MAX_LENGTH} tokens\")\n",
    "print(f\"   Batch size         : {BATCH_SIZE}\")\n",
    "print(f\"   Number of classes  : 3 (positive, neutral, negative)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ PREPROCESSING COMPLETE - DATASET READY FOR TRAINING\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Dictionary\n",
    "data_dictionary = pd.DataFrame({\n",
    "    'Column': [\n",
    "        'input',\n",
    "        'output',\n",
    "        'cleaned_input',\n",
    "        'sentiment',\n",
    "        'sentiment_encoded',\n",
    "        'text_length_chars',\n",
    "        'text_length_words'\n",
    "    ],\n",
    "    'Type': [\n",
    "        'string',\n",
    "        'string',\n",
    "        'string',\n",
    "        'string',\n",
    "        'int64',\n",
    "        'int64',\n",
    "        'int64'\n",
    "    ],\n",
    "    'Description': [\n",
    "        'Original raw text from dataset',\n",
    "        'Output label with prefix format (A:, B:, etc.)',\n",
    "        'Cleaned text (URLs, HTML, special chars removed)',\n",
    "        'Categorical sentiment label (positive/neutral/negative)',\n",
    "        'Numerical sentiment encoding (0=positive, 1=neutral, 2=negative)',\n",
    "        'Character count of cleaned text',\n",
    "        'Word count of cleaned text'\n",
    "    ],\n",
    "    'Example': [\n",
    "        'Amazing product! Love it ‚ù§Ô∏è',\n",
    "        'A: very positive',\n",
    "        'Amazing product Love it',\n",
    "        'positive',\n",
    "        '0',\n",
    "        '23',\n",
    "        '4'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DATA DICTIONARY\")\n",
    "print(\"=\" * 80)\n",
    "display(data_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "---\n",
    "# NEXT STEPS\n",
    "\n",
    "**Model Training Pipeline:**\n",
    "1. Load BertForSequenceClassification model\n",
    "2. Configure optimizer (AdamW) and learning rate scheduler\n",
    "3. Implement training loop with validation\n",
    "4. Evaluate on test set\n",
    "5. Save best model for inference\n",
    "\n",
    "**Available Variables:**\n",
    "- `train_dataloader` - Training data ready for model\n",
    "- `val_dataloader` - Validation data for hyperparameter tuning\n",
    "- `test_dataloader` - Test data for final evaluation\n",
    "- `tokenizer` - BERT tokenizer for inference\n",
    "- `df_train_oversampled` - Full training DataFrame\n",
    "- `df_val` - Validation DataFrame\n",
    "- `df_test` - Test DataFrame\n",
    "\n",
    "**Model Configuration:**\n",
    "- Input: Tokenized text (max_length=229)\n",
    "- Output: 3 classes (positive=0, neutral=1, negative=2)\n",
    "- Architecture: BERT-base-uncased\n",
    "- Batch size: 32"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
