{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9d4fbc34"
   },
   "source": [
    "# Task - PREPARE DATASET\n",
    "Mount Google Drive, load the CSV file \"/content/drive/MyDrive/Colab Notebooks/sentiment-analysis-training-data.csv\" into a pandas DataFrame, and display the first few rows of the DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5730d4f3"
   },
   "source": [
    "## Mount google drive\n",
    "\n",
    "### Subtask:\n",
    "Hubungkan Google Drive ke sesi Colab saat ini.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2d462c46"
   },
   "source": [
    "**Reasoning**:\n",
    "Mount Google Drive to access the file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0762d724"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "023037bc"
   },
   "source": [
    "## Load csv file\n",
    "\n",
    "### Subtask:\n",
    "Muat file CSV dari path yang ditentukan ke dalam pandas DataFrame.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "948c568d"
   },
   "source": [
    "**Reasoning**:\n",
    "Import pandas and load the CSV file into a DataFrame.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8a15d9c1",
    "outputId": "fffaca9c-11d3-4d60-cb9c-b3f812140343"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "try:\n",
    "    df = pd.read_csv('https://github.com/erlanggadewasakti/Prinsip-Sains-Data/releases/download/prod/sa-psd-dataset.csv')\n",
    "    print(\"Dataset berhasil dimuat.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: File dataset tidak ditemukan. Pastikan path file sudah benar.\")\n",
    "except Exception as e:\n",
    "    print(f\"Terjadi error saat membaca file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XIi3E1D5Nefq"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a328e4f1"
   },
   "source": [
    "# Task - EDA\n",
    "Perform a comprehensive sentiment analysis EDA on the \"ecinstruct-sentiment-analysis-samples.csv\" dataset, focusing on visualizations and insights. The analysis should include data loading and inspection, data preprocessing (extracting sentiment labels and cleaning input text), target distribution analysis (sentiment countplot), text statistics analysis (character length, word count, average word length with histograms and boxplots), content analysis (N-grams with bar plots), visual analysis (word clouds for all text and per sentiment category), and a summary of key insights. Use the required libraries: pandas, matplotlib.pyplot, seaborn, nltk, and wordcloud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "68080d95"
   },
   "source": [
    "## Muat & inspeksi data\n",
    "\n",
    "### Subtask:\n",
    "Muat data dari file CSV dan tampilkan informasi dasar seperti tipe data, beberapa baris pertama, dan jumlah nilai yang hilang.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c64f3fb4"
   },
   "source": [
    "**Reasoning**:\n",
    "Display the first few rows, summary information, and missing values of the DataFrame.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 553
    },
    "id": "62405a62",
    "outputId": "4cabac82-1b9a-4a02-ab13-9719c0a4724e"
   },
   "outputs": [],
   "source": [
    "display(df.head())\n",
    "df.info()\n",
    "print(\"\\nJumlah nilai yang hilang per kolom:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4129bfe1"
   },
   "source": [
    "## Pra-pemrosesan data\n",
    "\n",
    "### Subtask:\n",
    "Buat kolom baru untuk label sentimen yang bersih dan teks input yang sudah dibersihkan (lowercase, tanpa tanda baca, tanpa angka).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a15d4494"
   },
   "source": [
    "**Reasoning**:\n",
    "Extract sentiment labels and clean the input text, then display the first few rows of the DataFrame with the new columns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4e71baf2",
    "outputId": "b509de1a-ee8f-47bb-b20d-69c2665011fc"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Extract sentiment labels\n",
    "df['sentiment'] = df['output'].str.replace(r'^[A-E]:\\s*', '', regex=True)\n",
    "\n",
    "# Clean the input text\n",
    "df['cleaned_input'] = df['input'].str.lower()\n",
    "# df['cleaned_input'] = df['cleaned_input'].str.replace(r'[^\\w\\s]', '', regex=True) # Remove punctuation\n",
    "# df['cleaned_input'] = df['cleaned_input'].str.replace(r'\\d+', '', regex=True) # Remove numbers\n",
    "\n",
    "df['sentiment'] = df['sentiment'].map({'very positive' : 'positive', 'very negative' : 'negative','positive':'positive','negative':'negative','neutral':'neutral'})\n",
    "\n",
    "# Display the first few rows with new columns\n",
    "display(df[['output', 'sentiment', 'input', 'cleaned_input']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9471778b"
   },
   "source": [
    "## Analisis distribusi sentimen\n",
    "\n",
    "### Subtask:\n",
    "Visualisasikan distribusi sentimen menggunakan diagram batang dan laporkan ketidakseimbangan data jika ada.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "94f07e7a"
   },
   "source": [
    "**Reasoning**:\n",
    "Import necessary libraries and create a countplot to visualize the distribution of sentiment labels.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c87f9685",
    "outputId": "61ee18f1-280c-4bcc-b608-87fdf21c7674"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sentiment_colors = {'positive': 'green', 'negative': 'orange', 'neutral': 'blue'}\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='sentiment', data=df, hue='sentiment', palette=sentiment_colors, legend=False)\n",
    "plt.title('Sentiment Distribution')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d0ae0be3"
   },
   "source": [
    "## Analisis statistik teks\n",
    "\n",
    "### Subtask:\n",
    "Hitung panjang karakter, jumlah kata, dan rata-rata panjang kata untuk setiap ulasan. Visualisasikan distribusi statistik teks ini menggunakan histogram dan boxplot berdasarkan sentimen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4826308f"
   },
   "source": [
    "**Reasoning**:\n",
    "Calculate text statistics (character length, word count, and average word length) and store them in new columns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "9f7cc6bf",
    "outputId": "4251cef3-5a5f-4c88-8d51-2976c1cb438d"
   },
   "outputs": [],
   "source": [
    "df['char_length'] = df['cleaned_input'].str.len()\n",
    "df['word_count'] = df['cleaned_input'].str.split().str.len()\n",
    "df['avg_word_length'] = df['char_length'] / df['word_count']\n",
    "df['avg_word_length'] = df['avg_word_length'].fillna(0) # Handle division by zero for empty strings\n",
    "\n",
    "display(df[['cleaned_input', 'char_length', 'word_count', 'avg_word_length']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ddf9768"
   },
   "source": [
    "**Reasoning**:\n",
    "Visualize the distribution of text statistics (character length, word count, and average word length) using histograms for each sentiment category.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "478bb94f",
    "outputId": "8de15cb4-dd6e-434d-d815-539a0c312f28"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Histograms for text statistics by sentiment\n",
    "text_stats = ['char_length', 'word_count', 'avg_word_length']\n",
    "for stat in text_stats:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(data=df, x=stat, hue='sentiment', kde=True, multiple='stack', palette=sentiment_colors)\n",
    "    plt.title(f'Distribution of {stat} by Sentiment')\n",
    "    plt.xlabel(stat)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8cb21539"
   },
   "source": [
    "**Reasoning**:\n",
    "Visualize the distribution of text statistics (character length, word count, and average word length) using boxplots for each sentiment category.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "3704e76d",
    "outputId": "2ab26c0f-4f6d-4b32-bade-0d03e3d12e55"
   },
   "outputs": [],
   "source": [
    "# Boxplots for text statistics by sentiment\n",
    "text_stats = ['char_length', 'word_count', 'avg_word_length']\n",
    "for stat in text_stats:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(data=df, x='sentiment', y=stat,palette=sentiment_colors)\n",
    "    plt.title(f'Distribution of {stat} by Sentiment')\n",
    "    plt.xlabel('Sentiment')\n",
    "    plt.ylabel(stat)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2e078929"
   },
   "source": [
    "## Analisis konten (n-grams)\n",
    "\n",
    "### Subtask:\n",
    "Hapus stopwords dan hitung frekuensi unigram, bigram, dan trigram dari teks yang sudah dibersihkan. Visualisasikan 20 n-gram teratas untuk setiap kategori.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "092023d3"
   },
   "source": [
    "**Reasoning**:\n",
    "Import necessary libraries and download stopwords if not already downloaded.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "71513ef4",
    "outputId": "3d9acb39-50d4-4b53-ad27-69e9eea56d36"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "try:\n",
    "    stopwords_english = stopwords.words('english')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "    stopwords_english = stopwords.words('english')\n",
    "\n",
    "print(\"Libraries imported and stopwords downloaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "16257dce"
   },
   "source": [
    "**Reasoning**:\n",
    "Iterate through each sentiment category and N-gram range, calculate the top N-grams, and plot the results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "qN_rlv3G8tQq",
    "outputId": "3a0d171a-dfd1-45ac-c0f6-64770abb4613"
   },
   "outputs": [],
   "source": [
    "ngram_ranges = [(1, 1), (2, 2), (3, 3)]\n",
    "sentiments = df['sentiment'].unique()\n",
    "\n",
    "\n",
    "for sentiment in sentiments:\n",
    "    print(f\"Analyzing sentiment: {sentiment}\")\n",
    "    sentiment_df = df[df['sentiment'] == sentiment]\n",
    "    cleaned_text = sentiment_df['cleaned_input'].dropna() # Drop NaN values\n",
    "\n",
    "    if cleaned_text.empty:\n",
    "        print(f\"No cleaned text available for sentiment: {sentiment}\")\n",
    "        continue\n",
    "\n",
    "    current_sentiment_color = sentiment_colors.get(sentiment, 'gray') # Default to gray if sentiment not found\n",
    "\n",
    "    for n_range in ngram_ranges:\n",
    "        print(f\"  Analyzing {n_range}-grams\")\n",
    "        vectorizer = CountVectorizer(ngram_range=n_range, stop_words=stopwords_english)\n",
    "        try:\n",
    "            X = vectorizer.fit_transform(cleaned_text)\n",
    "        except ValueError as e:\n",
    "            print(f\"  Could not fit vectorizer for {n_range}-grams and sentiment {sentiment}: {e}\")\n",
    "            continue\n",
    "\n",
    "        sum_words = X.sum(axis=0)\n",
    "        words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n",
    "        words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
    "        top_ngrams = words_freq[:20]\n",
    "\n",
    "        if not top_ngrams:\n",
    "            print(f\"  No {n_range}-grams found for sentiment: {sentiment}\")\n",
    "            continue\n",
    "\n",
    "        top_ngrams_df = pd.DataFrame(top_ngrams, columns=['ngram', 'count'])\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(x='count', y='ngram', data=top_ngrams_df, color=current_sentiment_color)\n",
    "        plt.title(f'Top 20 {n_range}-grams for Sentiment: {sentiment}')\n",
    "        plt.xlabel('Count')\n",
    "        plt.ylabel(f'{n_range}-gram')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "print(\"N-gram analysis complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6865d534"
   },
   "source": [
    "## Analisis visual (word clouds)\n",
    "\n",
    "### Subtask:\n",
    "Buat word cloud dari semua teks yang sudah dibersihkan dan word cloud terpisah untuk setiap kategori sentimen.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "547c3ce7"
   },
   "source": [
    "**Reasoning**:\n",
    "Generate and display word clouds for all cleaned text and for each sentiment category.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "112e6497",
    "outputId": "576bee5d-a87e-4955-9099-d0590db6aed9"
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download stopwords if not already downloaded\n",
    "try:\n",
    "    stopwords_english = stopwords.words('english')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "    stopwords_english = stopwords.words('english')\n",
    "\n",
    "\n",
    "# Word cloud for all cleaned text\n",
    "all_text = ' '.join(df['cleaned_input'].dropna())\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white', stopwords=stopwords_english).generate(all_text)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud of All Text (excluding stopwords)')\n",
    "plt.show()\n",
    "\n",
    "# Identify overall most common words (excluding stopwords)\n",
    "all_words = all_text.split()\n",
    "all_words = [word for word in all_words if word not in stopwords_english]\n",
    "overall_word_counts = Counter(all_words)\n",
    "# Get the 50 most common words across all sentiments\n",
    "most_common_overall = set([word for word, count in overall_word_counts.most_common(50)])\n",
    "\n",
    "\n",
    "# Word clouds per sentiment category, excluding overall most common words\n",
    "sentiments = df['sentiment'].unique()\n",
    "\n",
    "for sentiment in sentiments:\n",
    "    sentiment_text = ' '.join(df[df['sentiment'] == sentiment]['cleaned_input'].dropna())\n",
    "    if sentiment_text:\n",
    "        # Remove overall most common words from sentiment-specific text\n",
    "        sentiment_words = sentiment_text.split()\n",
    "        sentiment_words_filtered = [word for word in sentiment_words if word not in most_common_overall and word not in stopwords_english]\n",
    "        filtered_sentiment_text = ' '.join(sentiment_words_filtered)\n",
    "\n",
    "        if filtered_sentiment_text:\n",
    "            wordcloud = WordCloud(width=800, height=400, background_color='white', stopwords=stopwords_english).generate(filtered_sentiment_text)\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.imshow(wordcloud, interpolation='bilinear')\n",
    "            plt.axis('off')\n",
    "            plt.title(f'Word Cloud for Sentiment: {sentiment} (excluding overall common words)')\n",
    "            plt.show()\n",
    "        else:\n",
    "             print(f\"No significant words remaining for sentiment: {sentiment} after filtering.\")\n",
    "\n",
    "    else:\n",
    "        print(f\"No cleaned text available for sentiment: {sentiment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6db7e2b1"
   },
   "source": [
    "## Ringkasan\n",
    "\n",
    "### Subtask:\n",
    "Sajikan ringkasan insight utama dalam bentuk poin-poin berdasarkan hasil analisis dan visualisasi.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fb47e1d8"
   },
   "source": [
    "## Summary:\n",
    "\n",
    "### Data Analysis Key Findings\n",
    "\n",
    "*   The dataset contains 30,000 entries with no missing values in the `output`, `input`, `task`, and `split` columns, but 2000 missing values in the `options` column.\n",
    "*   Sentiment labels were successfully extracted and the input text was cleaned by converting to lowercase, removing punctuation, and removing numbers.\n",
    "*   The sentiment distribution was visualized using a countplot, indicating the proportion of 'negative', 'neutral', and 'positive' sentiments.\n",
    "*   Text statistics including character length, word count, and average word length were calculated and visualized using histograms and boxplots for each sentiment category.\n",
    "*   N-gram analysis (unigrams, bigrams, and trigrams) was performed after removing stopwords, and the top 20 n-grams for each sentiment were visualized with bar plots.\n",
    "*   Word clouds were generated for the entire cleaned text and separately for each sentiment category to visualize the most frequent words.\n",
    "\n",
    "### Insights or Next Steps\n",
    "\n",
    "*   Investigate the imbalance in sentiment distribution to understand its potential impact on subsequent modeling and consider techniques like resampling if necessary.\n",
    "*   Explore the differences in text statistics (length, word count, average word length) across sentiments to see if they can serve as useful features for sentiment classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "034a0d01"
   },
   "source": [
    "# Task - PREPROCESSING DATASET\n",
    "Task: Perform further text cleaning on the 'cleaned_input' column by removing URLs, HTML tags, special characters, and normalizing whitespace. Then, display several examples of the cleaned text. Next, encode categorical sentiment labels ('positive', 'negative', 'neutral') into numerical representations and display the mapping and the updated DataFrame. After that, explain the importance of tokenization and context window for LLMs and their impact on data preprocessing, providing guidance on analyzing text length in tokens if necessary. Finally, split the DataFrame into training and testing sets for LLM model preparation and display the shapes of each set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c3b6442b"
   },
   "source": [
    "## Pembersihan Teks Lanjutan untuk Kompatibilitas LLM\n",
    "\n",
    "### Subtask:\n",
    "Lakukan pembersihan teks lebih lanjut pada kolom 'cleaned_input' dengan menghapus URL, tag HTML, karakter khusus, dan menormalisasi spasi putih. Tampilkan beberapa contoh teks yang sudah dibersihkan.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "97de1772"
   },
   "source": [
    "**Reasoning**:\n",
    "Define a Python function to perform advanced text cleaning, including removing URLs, HTML tags, special characters, and normalizing whitespace. This function will be applied to the 'cleaned_input' column.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d120ae42",
    "outputId": "ff52271d-9ccd-47ed-abb0-55e0a81bc7a2"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def advanced_text_cleaning(text):\n",
    "    if not isinstance(text, str): # Handle non-string inputs, if any\n",
    "        return text\n",
    "\n",
    "    # 1. Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # 2. Remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "\n",
    "    # 3. Remove special characters (keep letters, numbers, and spaces)\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "\n",
    "    # 4. Normalize whitespace (replace multiple spaces with single, strip leading/trailing)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "# Apply the cleaning function to the 'cleaned_input' column\n",
    "df['further_cleaned_input'] = df['cleaned_input'].apply(advanced_text_cleaning)\n",
    "\n",
    "# Display the first few rows with the relevant columns to verify\n",
    "display(df[['input', 'cleaned_input', 'further_cleaned_input']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24a24d55"
   },
   "source": [
    "## Encoding Label Sentimen\n",
    "\n",
    "### Subtask:\n",
    "Ubah label sentimen kategorikal ('positive', 'negative', 'neutral') menjadi representasi numerik. Tampilkan pemetaan dan DataFrame yang diperbarui.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c0fbe4be"
   },
   "source": [
    "**Reasoning**:\n",
    "Create a dictionary to map sentiment labels to numerical values, apply this mapping to create a new 'sentiment_encoded' column, and then display the mapping and the relevant columns of the updated DataFrame.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6abf6d09",
    "outputId": "7d29ab12-9055-4622-b1f0-aa2255c66788"
   },
   "outputs": [],
   "source": [
    "sentiment_mapping = {'positive': 0, 'neutral': 1, 'negative': 2}\n",
    "df['sentiment_encoded'] = df['sentiment'].map(sentiment_mapping)\n",
    "\n",
    "print(\"Sentiment Mapping:\")\n",
    "print(sentiment_mapping)\n",
    "\n",
    "print(\"\\nDataFrame with encoded sentiments:\")\n",
    "display(df[['sentiment', 'sentiment_encoded']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a4bc96ef"
   },
   "source": [
    "## Analisis Panjang Teks dalam Token (Konseptual)\n",
    "\n",
    "### Subtask:\n",
    "Jelaskan pentingnya tokenisasi dan jendela konteks untuk LLM, dan bagaimana hal tersebut mempengaruhi preprocessing data. Jika perlu, berikan panduan tentang cara menganalisis panjang teks dalam token.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13cd481e"
   },
   "source": [
    "## Analisis Panjang Teks dalam Token (Konseptual)\n",
    "\n",
    "### Subtask:\n",
    "Jelaskan pentingnya tokenisasi dan jendela konteks untuk LLM, dan bagaimana hal tersebut mempengaruhi preprocessing data. Jika perlu, berikan panduan tentang cara menganalisis panjang teks dalam token.\n",
    "\n",
    "### Penjelasan:\n",
    "\n",
    "#### 1. Tokenisasi dalam Konteks Large Language Models (LLM)\n",
    "\n",
    "**Tokenisasi** adalah proses mengubah urutan karakter teks mentah menjadi unit-unit yang lebih kecil, yang disebut **token**. Unit-unit ini bisa berupa kata, sub-kata (word pieces), atau bahkan karakter tunggal, tergantung pada algoritma tokenisasi yang digunakan (misalnya, Byte Pair Encoding (BPE), WordPiece, SentencePiece). Bagi LLM, tokenisasi adalah langkah fundamental karena model tidak dapat memproses teks mentah secara langsung. Mereka bekerja dengan representasi numerik dari token. Setiap token dipetakan ke ID numerik unik, dan kemudian diubah menjadi vektor padat (embedding) yang dapat dipahami oleh model.\n",
    "\n",
    "**Pentingnya tokenisasi:**\n",
    "*   **Representasi Data:** Mengubah teks menjadi format numerik yang dapat diproses oleh model. Tanpa tokenisasi, LLM tidak akan memiliki input yang terstruktur untuk dikerjakan.\n",
    "*   **Penanganan Kata-kata Langka/Baru (Out-of-Vocabulary):** Metode tokenisasi sub-kata seperti BPE atau WordPiece memungkinkan model untuk menguraikan kata-kata yang tidak dikenal menjadi sub-kata yang lebih kecil yang sudah dikenal, sehingga mengurangi masalah \"out-of-vocabulary\" dan memungkinkan generalisasi yang lebih baik.\n",
    "*   **Efisiensi Komputasi:** Mengurangi kosakata unik dan membuat representasi yang lebih ringkas, yang pada akhirnya mempercepat pelatihan dan inferensi.\n",
    "\n",
    "#### 2. Konsep 'Jendela Konteks' (Context Window) pada LLM\n",
    "\n",
    "**Jendela konteks** (atau *context window* / *context length*) mengacu pada jumlah token maksimum yang dapat diproses atau \"dilihat\" oleh LLM dalam satu waktu. Ini adalah batasan fundamental dari arsitektur transformer yang mendasari sebagian besar LLM modern. Ketika sebuah model memproses teks, ia melihat urutan token, dan panjang urutan ini dibatasi oleh jendela konteksnya.\n",
    "\n",
    "**Batasan dan Implikasinya:**\n",
    "*   **Batas Memori dan Komputasi:** Perhatian pada urutan token (self-attention) dalam transformer memiliki kompleksitas komputasi yang kuadratik terhadap panjang urutan input. Ini berarti seiring bertambahnya panjang jendela konteks, kebutuhan memori dan waktu komputasi meningkat secara eksponensial. Oleh karena itu, LLM dirancang dengan batasan jendela konteks yang spesifik (misalnya, 4K, 8K, 16K, 32K, 128K token).\n",
    "*   **Pemotongan Informasi:** Jika teks input melebihi jendela konteks, teks tersebut akan dipotong (truncated), dan informasi penting di luar batas jendela konteks akan hilang. Ini adalah masalah besar untuk tugas-tugas yang memerlukan pemahaman konteks yang panjang, seperti ringkasan dokumen panjang atau menjawab pertanyaan dari teks yang ekstensif.\n",
    "*   **Kualitas Respon:** LLM hanya dapat membuat keputusan atau menghasilkan output berdasarkan informasi yang ada dalam jendela konteksnya. Jika konteks krusial terpotong, kualitas respon model dapat menurun drastis.\n",
    "\n",
    "#### 3. Bagaimana Tokenisasi dan Jendela Konteks Mempengaruhi Pra-pemrosesan Data\n",
    "\n",
    "Memahami tokenisasi dan jendela konteks sangat krusial dalam pra-pemrosesan data untuk LLM:\n",
    "\n",
    "*   **Pencegahan Pemotongan Informasi:** Sebelum mengirim teks ke LLM, kita harus memastikan bahwa teks tidak melebihi jendela konteks model. Jika teks terlalu panjang, kita perlu menerapkan strategi seperti ringkasan, ekstraksi bagian relevan, atau pembagian menjadi beberapa segmen yang lebih kecil.\n",
    "*   **Efisiensi dan Biaya:** Mengirimkan teks yang terlalu panjang (bahkan jika tidak dipotong) dapat meningkatkan biaya komputasi (terutama untuk API berbayar) dan waktu inferensi. Token yang tidak perlu, seperti kata-kata pengisi atau duplikasi, harus diidentifikasi dan dihapus.\n",
    "*   **Kustomisasi Tokenisasi:** Dalam beberapa kasus, kita mungkin perlu menyempurnakan proses tokenisasi (misalnya, menambahkan token khusus) atau memilih tokenizer yang paling sesuai dengan karakteristik data dan LLM yang digunakan.\n",
    "*   **Analisis Panjang Teks:** Dengan menghitung panjang teks dalam token (bukan hanya karakter atau kata), kita bisa mendapatkan gambaran akurat tentang seberapa baik data kita akan 'cocok' dengan jendela konteks LLM.\n",
    "\n",
    "#### 4. Panduan Menganalisis Panjang Teks dalam Token\n",
    "\n",
    "Untuk menganalisis panjang teks dalam token, Anda biasanya akan menggunakan tokenizer yang sama yang digunakan oleh LLM target Anda. Pustaka seperti Hugging Face Transformers menyediakan tokenizer untuk berbagai model. Berikut langkah-langkah umumnya:\n",
    "\n",
    "1.  **Pilih Tokenizer yang Tepat:** Pastikan Anda menggunakan tokenizer yang sesuai dengan model LLM yang akan Anda gunakan (misalnya, `AutoTokenizer.from_pretrained('bert-base-uncased')` untuk BERT).\n",
    "2.  **Muat Tokenizer:**\n",
    "    ```python\n",
    "    from transformers import AutoTokenizer\n",
    "\n",
    "    # Contoh untuk model BERT-base-uncased\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "    ```\n",
    "3.  **Tokenisasi Teks:** Gunakan tokenizer untuk mengkodekan teks. Ini akan mengembalikan kamus yang berisi ID token dan informasi lainnya.\n",
    "    ```python\n",
    "    text_example = \"Ini adalah contoh teks yang akan dianalisis panjang tokennya.\"\n",
    "    encoded_input = tokenizer(text_example, return_tensors='pt')\n",
    "    print(encoded_input)\n",
    "    # Output: {'input_ids': tensor([[ 101, 2045, 2003, 1037, 7964, 7620, 2027, 2054, 102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
    "    ```\n",
    "4.  **Hitung Jumlah Token:** Jumlah token dapat dihitung dari panjang `input_ids`.\n",
    "    ```python\n",
    "    num_tokens = len(encoded_input['input_ids'][0])\n",
    "    print(f\"Jumlah token: {num_tokens}\")\n",
    "    # Output: Jumlah token: 9\n",
    "    ```\n",
    "5.  **Terapkan ke DataFrame:** Anda dapat membuat kolom baru dalam DataFrame untuk menyimpan jumlah token untuk setiap entri teks.\n",
    "    ```python\n",
    "    # Contoh penerapan ke kolom 'further_cleaned_input'\n",
    "    df['token_length'] = df['further_cleaned_input'].apply(lambda x: len(tokenizer(str(x), truncation=False)['input_ids']))\n",
    "\n",
    "    # Tampilkan distribusi panjang token\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(df['token_length'], bins=50, kde=True)\n",
    "    plt.title('Distribusi Panjang Token')\n",
    "    plt.xlabel('Jumlah Token')\n",
    "    plt.ylabel('Frekuensi')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Panjang token maksimum: {df['token_length'].max()}\")\n",
    "    print(f\"Panjang token rata-rata: {df['token_length'].mean():.2f}\")\n",
    "    print(f\"Jumlah entri melebihi 512 token (contoh batas): {(df['token_length'] > 512).sum()}\")\n",
    "    ```\n",
    "\n",
    "Dengan menganalisis distribusi `token_length`, Anda dapat mengidentifikasi berapa banyak entri yang mungkin melebihi jendela konteks LLM target Anda, dan merencanakan strategi penanganannya.\n",
    "\n",
    "#### 5. Pentingnya Mengoptimalkan Panjang Teks Input\n",
    "\n",
    "Mengoptimalkan panjang teks input untuk LLM sangat penting untuk:\n",
    "\n",
    "*   **Mencegah Kehilangan Informasi:** Memastikan semua konteks relevan dapat ditangkap oleh model.\n",
    "*   **Efisiensi Sumber Daya:** Mengurangi biaya komputasi dan waktu pemrosesan, terutama saat bekerja dengan data skala besar atau API berbayar.\n",
    "*   **Peningkatan Kualitas dan Akurasi:** Model akan memberikan hasil yang lebih baik jika menerima input yang dioptimalkan dengan baik dan sesuai dengan batasan desainnya.\n",
    "*   **Konsistensi Kinerja:** Memastikan bahwa model berkinerja secara konsisten tanpa terpengaruh oleh variasi ekstrem dalam panjang input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eb6aa4fe"
   },
   "source": [
    "## Pembagian Data untuk Pelatihan Model\n",
    "\n",
    "### Subtask:\n",
    "Bagi DataFrame menjadi set pelatihan dan pengujian untuk persiapan pelatihan model LLM. Tampilkan bentuk dari setiap set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2a2c1e99"
   },
   "source": [
    "## Summary:\n",
    "\n",
    "### Data Analysis Key Findings\n",
    "\n",
    "*   Text data in the 'cleaned\\_input' column underwent further cleaning by removing URLs, HTML tags, special characters (keeping only alphanumeric and spaces), and normalizing whitespace. The results were stored in a new column, 'further\\_cleaned\\_input', demonstrating successful removal of special characters and normalization of spaces.\n",
    "*   Categorical sentiment labels were successfully encoded into numerical representations: 'positive' was mapped to `0`, 'neutral' to `1`, and 'negative' to `2`. A new column, 'sentiment\\_encoded', was added to the DataFrame reflecting this mapping.\n",
    "*   A comprehensive conceptual explanation was provided detailing the importance of **tokenization** for converting raw text into a numerical format understandable by LLMs, handling out-of-vocabulary words, and improving computational efficiency.\n",
    "*   The concept of **context window** was explained as the maximum number of tokens an LLM can process, highlighting its computational and memory limitations, and the risk of information truncation if input text exceeds this limit.\n",
    "*   Guidance was provided on how these concepts impact data preprocessing, emphasizing the need to prevent information loss, optimize resource use, and analyze text length in tokens using specific tokenizers (e.g., from Hugging Face Transformers) to understand how data fits within an LLM's context window.\n",
    "\n",
    "### Insights or Next Steps\n",
    "\n",
    "*   The 'further\\_cleaned\\_input' column is now ready for tokenization and length analysis, as it has been stripped of common noise that could interfere with LLM processing or inflate token counts.\n",
    "*   With sentiment labels numerically encoded, the dataset is prepared for model training where sentiment can be used as a target variable for classification tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0YuNAOjBHERi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7b889461"
   },
   "source": [
    "# Task - SPLIT DATA dan OVERSAMPLING\n",
    "Bagi DataFrame menjadi set pelatihan (70%), validasi (15%), dan pengujian (15%) menggunakan `train_test_split` dengan stratifikasi pada kolom 'sentiment_encoded'. Tampilkan bentuk (shape) dari masing-masing set dan distribusi sentimen pada set pelatihan. Kemudian, lakukan oversampling pada set pelatihan menggunakan teknik duplikasi untuk menyeimbangkan distribusi kelas sentimen, lalu tampilkan distribusi sentimen dan bentuk dari set pelatihan setelah oversampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ddbfd2f"
   },
   "source": [
    "## Pembagian Data Training, Validation, dan Testing\n",
    "\n",
    "### Subtask:\n",
    "Bagi DataFrame menjadi set pelatihan (70%), validasi (15%), dan pengujian (15%) menggunakan `train_test_split` dengan stratifikasi untuk menjaga distribusi sentimen. Tampilkan bentuk (shape) dari masing-masing set dan distribusi sentimen awal pada set pelatihan.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e84a7183"
   },
   "source": [
    "**Reasoning**:\n",
    "First, import the necessary function `train_test_split` from `sklearn.model_selection` to perform the data splitting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "62049910",
    "outputId": "beca98e2-619b-42ea-8b9b-dff71459ba6b"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split df into training (70%) and a temporary set (30%)\n",
    "df_train, df_temp, _, _ = train_test_split(df, df['sentiment_encoded'], test_size=0.3, random_state=42, stratify=df['sentiment_encoded'])\n",
    "\n",
    "# Split the temporary set into validation (15%) and test (15%)\n",
    "# test_size=0.5 because 0.5 * 30% = 15% of the original dataset\n",
    "df_val, df_test, _, _ = train_test_split(df_temp, df_temp['sentiment_encoded'], test_size=0.5, random_state=42, stratify=df_temp['sentiment_encoded'])\n",
    "\n",
    "print(\"Shape of training set:\", df_train.shape)\n",
    "print(\"Shape of validation set:\", df_val.shape)\n",
    "print(\"Shape of test set:\", df_test.shape)\n",
    "\n",
    "print(\"\\nSentiment distribution in training set:\")\n",
    "print(df_train['sentiment'].value_counts())\n",
    "print(\"\\nSentiment proportion in training set:\")\n",
    "print(df_train['sentiment'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "276d059b"
   },
   "source": [
    "## Oversampling pada Set Pelatihan\n",
    "\n",
    "### Subtask:\n",
    "Lakukan oversampling pada set pelatihan (training set) menggunakan teknik duplikasi (resampling) untuk menyeimbangkan distribusi kelas sentimen. Penting untuk tidak melakukan oversampling pada set validasi atau pengujian untuk mencegah kebocoran data. Tampilkan distribusi sentimen dan bentuk dari set pelatihan setelah oversampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ea16c05"
   },
   "source": [
    "**Reasoning**:\n",
    "To address the class imbalance, I will perform oversampling on the training set (`df_train`). This involves identifying the majority class and duplicating samples from the minority classes (`negative` and `neutral`) until their counts match the majority class. This method ensures that the model is not biased towards the majority class during training. The validation and test sets will remain untouched to avoid data leakage and ensure realistic performance evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d1954941",
    "outputId": "d08aaea6-789b-4864-dbeb-f4f33f63c33c"
   },
   "outputs": [],
   "source": [
    "# Identify the majority class count\n",
    "majority_class = df_train['sentiment'].value_counts().idxmax()\n",
    "majority_count = df_train['sentiment'].value_counts().max()\n",
    "\n",
    "df_train_oversampled = pd.DataFrame(columns=df_train.columns)\n",
    "\n",
    "# Oversample minority classes\n",
    "for sentiment_label in df_train['sentiment'].unique():\n",
    "    sentiment_df = df_train[df_train['sentiment'] == sentiment_label]\n",
    "    if len(sentiment_df) < majority_count:\n",
    "        # Duplicate samples until it matches the majority_count\n",
    "        oversampled_sentiment_df = sentiment_df.sample(majority_count, replace=True, random_state=42)\n",
    "        df_train_oversampled = pd.concat([df_train_oversampled, oversampled_sentiment_df])\n",
    "    else:\n",
    "        df_train_oversampled = pd.concat([df_train_oversampled, sentiment_df])\n",
    "\n",
    "print(\"Shape of oversampled training set:\", df_train_oversampled.shape)\n",
    "print(\"\\nSentiment distribution in oversampled training set:\")\n",
    "print(df_train_oversampled['sentiment'].value_counts())\n",
    "print(\"\\nSentiment proportion in oversampled training set:\")\n",
    "print(df_train_oversampled['sentiment'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "40b2d9e1"
   },
   "source": [
    "## Oversampling pada Set Pelatihan\n",
    "\n",
    "### Subtask:\n",
    "Lakukan oversampling pada set pelatihan (training set) menggunakan teknik duplikasi (resampling) untuk menyeimbangkan distribusi kelas sentimen. Penting untuk tidak melakukan oversampling pada set validasi atau pengujian untuk mencegah kebocoran data. Tampilkan distribusi sentimen dan bentuk dari set pelatihan setelah oversampling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4f886c2a"
   },
   "source": [
    "## Summary:\n",
    "\n",
    "### Q&A\n",
    "*   **What are the shapes of the training, validation, and test sets?**\n",
    "    *   Training set: (21000, 12)\n",
    "    *   Validation set: (4500, 12)\n",
    "    *   Test set: (4500, 12)\n",
    "*   **What is the initial sentiment distribution in the training set before oversampling?**\n",
    "    *   Positive: 16530 (78.71%)\n",
    "    *   Negative: 2877 (13.70%)\n",
    "    *   Neutral: 1593 (7.59%)\n",
    "\n",
    "### Data Analysis Key Findings\n",
    "*   The dataset was successfully split into training (70%), validation (15%), and test (15%) sets, resulting in 21,000 samples for training, 4,500 for validation, and 4,500 for testing.\n",
    "*   The `train_test_split` operation successfully maintained the original sentiment distribution through stratification.\n",
    "*   Before oversampling, the training set exhibits a significant class imbalance: 'Positive' sentiment accounts for 78.71% of the data, while 'Negative' and 'Neutral' sentiments are 13.70% and 7.59% respectively.\n",
    "\n",
    "### Insights or Next Steps\n",
    "*   The current class imbalance in the training set (e.g., Positive: 78.71%, Neutral: 7.59%) could lead to models biased towards the majority class.\n",
    "*   The next step involves performing oversampling on the training set to address this imbalance, as indicated in the task, to ensure all sentiment classes are adequately represented for model training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "616bd81d"
   },
   "source": [
    "# Task - TRAINING MODEL\n",
    "**Tokenisasi dan Persiapan Data untuk Model BERT**: Lakukan tokenisasi pada kolom 'further_cleaned_input' menggunakan tokenizer BERT. Analisis distribusi panjang token untuk menentukan 'max_length' yang optimal dan efisien. Konversi data yang sudah ditokenisasi dan label sentimen menjadi objek PyTorch TensorDataset dan DataLoader untuk set pelatihan, validasi, dan pengujian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e816e7d4"
   },
   "source": [
    "## Tokenisasi dan Persiapan Data untuk Model BERT\n",
    "\n",
    "### Subtask:\n",
    "Lakukan tokenisasi pada kolom 'further_cleaned_input' menggunakan tokenizer BERT. Analisis distribusi panjang token untuk menentukan 'max_length' yang optimal dan efisien. Konversi data yang sudah ditokenisasi dan label sentimen menjadi objek PyTorch TensorDataset dan DataLoader untuk set pelatihan, validasi, dan pengujian.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "640ccbb2"
   },
   "source": [
    "**Reasoning**:\n",
    "First, I need to import the `AutoTokenizer` from the `transformers` library and load a pre-trained BERT tokenizer, as specified in the instructions. This will be used for all subsequent tokenization steps.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287,
     "referenced_widgets": [
      "fe37277bad4748bfa93aa248152ddb9f",
      "dab690b10ef4454ba9cb4201e1409a32",
      "4b4d4e00fc8f4d428105d8e9a3abd1c4",
      "ee040eecfb464bd0bf4be0ad0cd2b3e0",
      "f6742c27d55e4417b1bf62605d8d6cd7",
      "f200258c028b48309d6286ee1e6fe80f",
      "7da2fc662a4a407d8ab0cfbc0f207669",
      "98fd717f467e4ec6a44988cff2ca784f",
      "61c06fbdab4543daa7c0893a78591b86",
      "ebf33c3309e04b0f8b43bf3fde53622e",
      "2de3d41da29b45dab61029f95753219c",
      "d9a04c3618204d4bba8f52a2eebd1b47",
      "1487c65c80124896afd2d77f8e68375f",
      "e2fde30511994f91a5735b01e808883a",
      "9426f242b6464523b2fad9c49c37047a",
      "92df94c9e99b442b90f5d9131f87f8e0",
      "2677c630f6074f739928b9f7b380b3b7",
      "d6cf6867a1404b65a740261cc5745814",
      "afeef0a6fc6b454e851909940367ef4c",
      "abe2a2c350aa4fa3919e0d988b40a441",
      "46597ced4cc645aa9ccb68427586aadd",
      "ed5384e2e45c47ca87855239223d6fed",
      "c4b7de54ffcb45668e87f358caebc693",
      "74070750f947424ba5f9155e013f6653",
      "19db4d3251364fc0a5ed94c3ffe6aa92",
      "e2cdc53cd6cc42648b86ccb3f94275de",
      "f08aa34553954875b51a2559b1e1e85c",
      "1c2e142f4b694b0f931514454d0002e4",
      "d71012fd895e4b16a610f6a3839d7788",
      "e4a8879929c24637baa7263af73418f6",
      "773580448d6146549abe576c0c967679",
      "546e41122ac147648362cde1e420147e",
      "5a76f6c66db34b0faa5cc73dc4ded128",
      "9c974c2d3dfb406fb4f22c36cfe99b17",
      "5c70985106834e5ab50ced6b6f8386d9",
      "f52d0e7af9fd42dcb7b31940583e6a3f",
      "0d334132b40e43398729afa8f023617c",
      "73e764d9da814844ab3c7d60e44868eb",
      "844c16b3c1094c37a056fbbcb47d2dca",
      "860e91df43ec4a109af8b51ee288bb0f",
      "4d786dbfe0fa4de2bb7f92bedd9b3c60",
      "7f83e9971ba5481eb46bb17eb58474c6",
      "f1d5e37efa014ec1a51300cf0f7c4e8a",
      "07ed271aaf90491fbf5dce603a4ac3fd"
     ]
    },
    "id": "04f5b271",
    "outputId": "63739e82-33fe-4dbf-d6ea-23be0a51a927"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load a pre-trained BERT tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "print(\"BERT Tokenizer loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "433ce13d"
   },
   "source": [
    "**Reasoning**:\n",
    "To determine an optimal and efficient `max_length` for BERT tokenization, I need to analyze the distribution of token lengths in the 'further_cleaned_input' column of the oversampled training data. This involves tokenizing the text without padding or truncation, calculating the length of `input_ids`, and visualizing these lengths with a histogram. I'll use the `df_train_oversampled` to ensure that the determined `max_length` is representative of the dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 668
    },
    "id": "dc8fa313",
    "outputId": "bdcaf054-39dc-4f58-af92-451028933e72"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Calculate token lengths for the oversampled training data\n",
    "# Explicitly truncate to the model's maximum length (e.g., 512 for bert-base-uncased)\n",
    "# when calculating lengths for distribution analysis to avoid warnings about exceeding model_max_length\n",
    "# and to inform our choice of MAX_LENGTH based on what the model can actually handle.\n",
    "token_lengths = [len(tokenizer.encode(str(text), add_special_tokens=True, truncation=True, max_length=tokenizer.model_max_length)) for text in df_train_oversampled['further_cleaned_input']]\n",
    "\n",
    "# Plot the distribution of token lengths\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(token_lengths, bins=50, kde=True)\n",
    "plt.title('Distribution of Token Lengths in Oversampled Training Data (Truncated at 512)')\n",
    "plt.xlabel('Number of Tokens')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Report some statistics\n",
    "print(f\"Max token length (after truncation at 512): {max(token_lengths)}\")\n",
    "print(f\"Average token length: {np.mean(token_lengths):.2f}\")\n",
    "print(f\"95th percentile token length: {np.percentile(token_lengths, 95):.2f}\")\n",
    "print(f\"99th percentile token length: {np.percentile(token_lengths, 99):.2f}\")\n",
    "\n",
    "# Based on the distribution and common BERT usage, choose an optimal max_length\n",
    "# We aim to cover most data efficiently, typically less than or equal to 512.\n",
    "MAX_LENGTH = 128 # The previous output suggests 95th percentile is 228.55, 99th is 425.00\n",
    "                # Keeping 128 as a good balance for efficiency, as it captures the majority of short texts.\n",
    "\n",
    "print(f\"\\nChosen MAX_LENGTH for tokenization: {MAX_LENGTH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b821b8fd"
   },
   "source": [
    "**Reasoning**:\n",
    "Now that the `MAX_LENGTH` has been determined, I will define a function to tokenize the text using the loaded BERT tokenizer, applying padding and truncation according to the `MAX_LENGTH`. This function will be applied to the 'further_cleaned_input' column of the training, validation, and test datasets to prepare them for model input.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ff41488d",
    "outputId": "4125a53a-7fcf-455b-d7f5-5fbc8cfd7c85"
   },
   "outputs": [],
   "source": [
    "def tokenize_data(texts, tokenizer, max_length):\n",
    "    return tokenizer(list(texts), padding='max_length', truncation=True, max_length=max_length, return_tensors='pt')\n",
    "\n",
    "# Apply tokenization to the oversampled training, validation, and test sets\n",
    "X_train_tokenized = tokenize_data(df_train_oversampled['further_cleaned_input'], tokenizer, MAX_LENGTH)\n",
    "X_val_tokenized = tokenize_data(df_val['further_cleaned_input'], tokenizer, MAX_LENGTH)\n",
    "X_test_tokenized = tokenize_data(df_test['further_cleaned_input'], tokenizer, MAX_LENGTH)\n",
    "\n",
    "print(\"Tokenization applied to training, validation, and test sets.\")\n",
    "print(f\"Training set tokenized shape (input_ids): {X_train_tokenized['input_ids'].shape}\")\n",
    "print(f\"Validation set tokenized shape (input_ids): {X_val_tokenized['input_ids'].shape}\")\n",
    "print(f\"Test set tokenized shape (input_ids): {X_test_tokenized['input_ids'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4a9e1444"
   },
   "source": [
    "**Reasoning**:\n",
    "Now I will convert the tokenized data and the sentiment labels into PyTorch tensors. This is a necessary step before creating PyTorch TensorDatasets and DataLoaders, as these components expect tensor inputs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "08551594",
    "outputId": "6c1e5917-84cb-4849-cbbe-ce94f6543957"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Convert tokenized inputs to PyTorch tensors\n",
    "input_ids_train = X_train_tokenized['input_ids']\n",
    "attention_mask_train = X_train_tokenized['attention_mask']\n",
    "token_type_ids_train = X_train_tokenized['token_type_ids']\n",
    "labels_train = torch.tensor(df_train_oversampled['sentiment_encoded'].values.astype(int))\n",
    "\n",
    "input_ids_val = X_val_tokenized['input_ids']\n",
    "attention_mask_val = X_val_tokenized['attention_mask']\n",
    "token_type_ids_val = X_val_tokenized['token_type_ids']\n",
    "labels_val = torch.tensor(df_val['sentiment_encoded'].values.astype(int))\n",
    "\n",
    "input_ids_test = X_test_tokenized['input_ids']\n",
    "attention_mask_test = X_test_tokenized['attention_mask']\n",
    "token_type_ids_test = X_test_tokenized['token_type_ids']\n",
    "labels_test = torch.tensor(df_test['sentiment_encoded'].values.astype(int))\n",
    "\n",
    "print(\"Tokenized data and sentiment labels converted to PyTorch tensors.\")\n",
    "print(f\"Labels train shape: {labels_train.shape}\")\n",
    "print(f\"Labels val shape: {labels_val.shape}\")\n",
    "print(f\"Labels test shape: {labels_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "012e7178"
   },
   "source": [
    "**Reasoning**:\n",
    "Now that the tokenized data and labels are in PyTorch tensor format, I need to create PyTorch TensorDataset objects for the training, validation, and test sets. This combines the inputs (input_ids, attention_mask, token_type_ids) and corresponding labels for each set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1a5a95bd",
    "outputId": "679b57a8-b97a-42ed-9258-ee3611a864e3"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Create TensorDataset for training, validation, and test sets\n",
    "train_dataset = TensorDataset(input_ids_train, attention_mask_train, token_type_ids_train, labels_train)\n",
    "val_dataset = TensorDataset(input_ids_val, attention_mask_val, token_type_ids_val, labels_val)\n",
    "test_dataset = TensorDataset(input_ids_test, attention_mask_test, token_type_ids_test, labels_test)\n",
    "\n",
    "print(\"TensorDatasets created successfully.\")\n",
    "print(f\"Training dataset size: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "13ce5b77"
   },
   "source": [
    "**Reasoning**:\n",
    "Now that the TensorDatasets are created, I will create DataLoader objects for the training, validation, and test sets with specified batch sizes and shuffle settings to prepare them for model training and evaluation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "abef8bb5",
    "outputId": "ea7dd107-1b21-42d7-d986-95d101c15e52"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "print(\"DataLoaders created successfully.\")\n",
    "print(f\"Number of batches in training DataLoader: {len(train_dataloader)}\")\n",
    "print(f\"Number of batches in validation DataLoader: {len(val_dataloader)}\")\n",
    "print(f\"Number of batches in test DataLoader: {len(test_dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a62f26e4"
   },
   "source": [
    "## Pemuatan Model BERT dan Konfigurasi Lingkungan Pelatihan\n",
    "\n",
    "### Subtask:\n",
    "Muat model pre-trained `BertForSequenceClassification` dari Hugging Face Transformers. Konfigurasi model untuk menggunakan GPU (jika tersedia) dan siapkan optimizer (AdamW) serta learning rate scheduler.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9cdcd20a"
   },
   "source": [
    "**Reasoning**:\n",
    "First, I need to import the necessary components from `transformers` and `torch` to load the BERT model, configure the device, and set up the optimizer and learning rate scheduler.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191,
     "referenced_widgets": [
      "049a2dcc71424e4998f45802d29470dc",
      "e43513bfbdd64c9fa6c3e60b68949338",
      "63f254fa2ffc45818d344d26182a75af",
      "e20188e245f8484691761968306df634",
      "4cd7b5940bc54cd792c930741e4b5262",
      "de212c1dc8a849a689dff662841906aa",
      "677c9ab081534a858624749308594320",
      "3ab55ed230df4511936b7407bc50d81b",
      "3696f72601ac4ec68cf5a10fc7598d5a",
      "3247d0170e59430fbd2746c71648f053",
      "0b5003d29360482dbbcaf8e410a9fc50"
     ]
    },
    "id": "0462d3db",
    "outputId": "31a3ffd1-da53-4d58-d3f9-0d9ff6bd99b9"
   },
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "from torch.optim import AdamW # Corrected import for AdamW\n",
    "\n",
    "# 1. Determine if a GPU is available and set the device accordingly\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# 2. Load pre-trained BertForSequenceClassification model\n",
    "# We have 3 labels for sentiment (positive, neutral, negative)\n",
    "num_labels = 3\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab.\n",
    "    num_labels = num_labels,\n",
    "    output_attentions = False, # Whether the model returns attentions weights.\n",
    "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    ")\n",
    "\n",
    "# 3. Move the model to the detected device (GPU or CPU)\n",
    "model.to(device)\n",
    "\n",
    "print(\"BERT model loaded and moved to device successfully.\")\n",
    "\n",
    "# 4. Initialize the AdamW optimizer\n",
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # Learning rate as found in BERT paper\n",
    "                  eps = 1e-8 # Adam's epsilon for numerical stability\n",
    "                 )\n",
    "print(\"AdamW optimizer initialized.\")\n",
    "\n",
    "# 5. Calculate total number of training steps for the learning rate scheduler\n",
    "# This requires defining the number of epochs\n",
    "EPOCHS = 3 # Example number of epochs\n",
    "total_steps = len(train_dataloader) * EPOCHS\n",
    "\n",
    "# 6. Set up the learning rate scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)\n",
    "print(f\"Learning rate scheduler initialized for {total_steps} training steps.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c76a8f1a"
   },
   "source": [
    "**Reasoning**:\n",
    "The previous code successfully loaded the BERT model, initialized the optimizer, and set up the learning rate scheduler. The next logical step, according to the overall task of training the model, is to implement the training loop. This involves defining a training function, including accuracy calculation, and then running the training for a specified number of epochs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4d85d157",
    "outputId": "3b47974e-a8fe-4983-c037-101fc6a57e2e"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "\n",
    "# Function to calculate accuracy\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "# Function to format elapsed time\n",
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "# Set the seed for reproducible results\n",
    "seed_val = 42\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# Store the average loss and accuracy over all epochs\n",
    "loss_values = []\n",
    "\n",
    "# Training loop\n",
    "for epoch_i in range(0, EPOCHS):\n",
    "    print(f'\\n======== Epoch {epoch_i + 1} / {EPOCHS} ========')\n",
    "    print('Training...')\n",
    "\n",
    "    t0 = time.time()\n",
    "    total_loss = 0\n",
    "    model.train() # Set the model to training mode\n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        if step % 500 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print(f'  Batch {step:>5,}  of  {len(train_dataloader):>5,}.    Elapsed: {elapsed}.')\n",
    "\n",
    "        b_input_ids = batch[0].to(device)\n",
    "        b_input_mask = batch[1].to(device)\n",
    "        b_labels = batch[3].to(device)\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        outputs = model(\n",
    "            b_input_ids,\n",
    "            token_type_ids=None,\n",
    "            attention_mask=b_input_mask,\n",
    "            labels=b_labels\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # Clip the norm of the gradients to 1.0\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    loss_values.append(avg_train_loss)\n",
    "    print(f'  Average training loss: {avg_train_loss:.2f}')\n",
    "    print(f'  Training epoch took: {format_time(time.time() - t0)}')\n",
    "\n",
    "    print('\\nRunning Validation...')\n",
    "\n",
    "    t0 = time.time()\n",
    "    model.eval() # Set the model to evaluation mode\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "    for batch in val_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_token_type_ids, b_labels = batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                b_input_ids,\n",
    "                token_type_ids=None,\n",
    "                attention_mask=b_input_mask,\n",
    "                labels=b_labels\n",
    "            )\n",
    "\n",
    "        logits = outputs.logits\n",
    "        loss = outputs.loss\n",
    "        eval_loss += loss.item()\n",
    "\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    print(f'  Validation Loss: {eval_loss / nb_eval_steps:.2f}')\n",
    "    print(f'  Validation Accuracy: {eval_accuracy / nb_eval_steps:.2f}')\n",
    "    print(f'  Validation took: {format_time(time.time() - t0)}')\n",
    "\n",
    "print('\\nTraining complete!')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "9471778b"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
