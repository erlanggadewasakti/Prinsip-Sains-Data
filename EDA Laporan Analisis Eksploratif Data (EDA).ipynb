{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis - Exploratory Data Analysis (EDA)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "023037bc"
   },
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8a15d9c1",
    "outputId": "fffaca9c-11d3-4d60-cb9c-b3f812140343"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "try:\n",
    "    df = pd.read_csv('https://github.com/erlanggadewasakti/Prinsip-Sains-Data/releases/download/prod/sa-psd-dataset.csv')\n",
    "    print(\"Dataset berhasil dimuat.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: File dataset tidak ditemukan. Pastikan path file sudah benar.\")\n",
    "except Exception as e:\n",
    "    print(f\"Terjadi error saat membaca file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a328e4f1"
   },
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "68080d95"
   },
   "source": [
    "## Data Loading & Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 553
    },
    "id": "62405a62",
    "outputId": "4cabac82-1b9a-4a02-ab13-9719c0a4724e"
   },
   "outputs": [],
   "source": [
    "display(df.head())\n",
    "df.info()\n",
    "print(\"\\nJumlah nilai yang hilang per kolom:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4129bfe1"
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4e71baf2",
    "outputId": "b509de1a-ee8f-47bb-b20d-69c2665011fc"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Extract sentiment labels from output column\n",
    "df['sentiment'] = df['output'].str.replace(r'^[A-E]:\\s*', '', regex=True)\n",
    "\n",
    "# Clean and normalize input text\n",
    "df['cleaned_input'] = df['input'].str.lower()\n",
    "\n",
    "# Map sentiment categories\n",
    "df['sentiment'] = df['sentiment'].map({\n",
    "    'very positive': 'positive',\n",
    "    'very negative': 'negative',\n",
    "    'positive': 'positive',\n",
    "    'negative': 'negative',\n",
    "    'neutral': 'neutral'\n",
    "})\n",
    "\n",
    "display(df[['output', 'sentiment', 'input', 'cleaned_input']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9471778b"
   },
   "source": [
    "## Sentiment Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c87f9685",
    "outputId": "61ee18f1-280c-4bcc-b608-87fdf21c7674"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sentiment_colors = {'positive': 'green', 'negative': 'orange', 'neutral': 'blue'}\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='sentiment', data=df, hue='sentiment', palette=sentiment_colors, legend=False)\n",
    "plt.title('Sentiment Distribution')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d0ae0be3"
   },
   "source": [
    "## Text Statistics Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "9f7cc6bf",
    "outputId": "4251cef3-5a5f-4c88-8d51-2976c1cb438d"
   },
   "outputs": [],
   "source": [
    "# Calculate text statistics\n",
    "df['char_length'] = df['cleaned_input'].str.len()\n",
    "df['word_count'] = df['cleaned_input'].str.split().str.len()\n",
    "df['avg_word_length'] = df['char_length'] / df['word_count']\n",
    "df['avg_word_length'] = df['avg_word_length'].fillna(0)\n",
    "\n",
    "display(df[['cleaned_input', 'char_length', 'word_count', 'avg_word_length']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "478bb94f",
    "outputId": "8de15cb4-dd6e-434d-d815-539a0c312f28"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Visualize text statistics distribution using histograms\n",
    "text_stats = ['char_length', 'word_count', 'avg_word_length']\n",
    "for stat in text_stats:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(data=df, x=stat, hue='sentiment', kde=True, multiple='stack', palette=sentiment_colors)\n",
    "    plt.title(f'Distribution of {stat} by Sentiment')\n",
    "    plt.xlabel(stat)\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "3704e76d",
    "outputId": "2ab26c0f-4f6d-4b32-bade-0d03e3d12e55"
   },
   "outputs": [],
   "source": [
    "# Visualize text statistics distribution using boxplots\n",
    "text_stats = ['char_length', 'word_count', 'avg_word_length']\n",
    "for stat in text_stats:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(data=df, x='sentiment', y=stat, palette=sentiment_colors)\n",
    "    plt.title(f'Distribution of {stat} by Sentiment')\n",
    "    plt.xlabel('Sentiment')\n",
    "    plt.ylabel(stat)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2e078929"
   },
   "source": [
    "## Content Analysis (N-grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "71513ef4",
    "outputId": "3d9acb39-50d4-4b53-ad27-69e9eea56d36"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Download stopwords\n",
    "try:\n",
    "    stopwords_english = stopwords.words('english')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "    stopwords_english = stopwords.words('english')\n",
    "\n",
    "print(\"Libraries imported and stopwords downloaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "qN_rlv3G8tQq",
    "outputId": "3a0d171a-dfd1-45ac-c0f6-64770abb4613"
   },
   "outputs": [],
   "source": [
    "# Analyze N-grams for each sentiment\n",
    "ngram_ranges = [(1, 1), (2, 2), (3, 3)]\n",
    "sentiments = df['sentiment'].unique()\n",
    "\n",
    "for sentiment in sentiments:\n",
    "    print(f\"Analyzing sentiment: {sentiment}\")\n",
    "    sentiment_df = df[df['sentiment'] == sentiment]\n",
    "    cleaned_text = sentiment_df['cleaned_input'].dropna()\n",
    "\n",
    "    if cleaned_text.empty:\n",
    "        print(f\"No cleaned text available for sentiment: {sentiment}\")\n",
    "        continue\n",
    "\n",
    "    current_sentiment_color = sentiment_colors.get(sentiment, 'gray')\n",
    "\n",
    "    for n_range in ngram_ranges:\n",
    "        print(f\"  Analyzing {n_range}-grams\")\n",
    "        vectorizer = CountVectorizer(ngram_range=n_range, stop_words=stopwords_english)\n",
    "\n",
    "        try:\n",
    "            X = vectorizer.fit_transform(cleaned_text)\n",
    "        except ValueError as e:\n",
    "            print(f\"  Could not fit vectorizer for {n_range}-grams and sentiment {sentiment}: {e}\")\n",
    "            continue\n",
    "\n",
    "        sum_words = X.sum(axis=0)\n",
    "        words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]\n",
    "        words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
    "        top_ngrams = words_freq[:20]\n",
    "\n",
    "        if not top_ngrams:\n",
    "            print(f\"  No {n_range}-grams found for sentiment: {sentiment}\")\n",
    "            continue\n",
    "\n",
    "        top_ngrams_df = pd.DataFrame(top_ngrams, columns=['ngram', 'count'])\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(x='count', y='ngram', data=top_ngrams_df, color=current_sentiment_color)\n",
    "        plt.title(f'Top 20 {n_range}-grams for Sentiment: {sentiment}')\n",
    "        plt.xlabel('Count')\n",
    "        plt.ylabel(f'{n_range}-gram')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "print(\"N-gram analysis complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6865d534"
   },
   "source": [
    "## Visual Analysis (Word Clouds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "112e6497",
    "outputId": "576bee5d-a87e-4955-9099-d0590db6aed9"
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download stopwords\n",
    "try:\n",
    "    stopwords_english = stopwords.words('english')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "    stopwords_english = stopwords.words('english')\n",
    "\n",
    "# Generate word cloud for all text\n",
    "all_text = ' '.join(df['cleaned_input'].dropna())\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white',\n",
    "                     stopwords=stopwords_english).generate(all_text)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud of All Text (excluding stopwords)')\n",
    "plt.show()\n",
    "\n",
    "# Identify overall most common words (excluding stopwords)\n",
    "all_words = all_text.split()\n",
    "all_words = [word for word in all_words if word not in stopwords_english]\n",
    "overall_word_counts = Counter(all_words)\n",
    "most_common_overall = set([word for word, count in overall_word_counts.most_common(50)])\n",
    "\n",
    "# Generate word clouds per sentiment (excluding overall common words)\n",
    "sentiments = df['sentiment'].unique()\n",
    "\n",
    "for sentiment in sentiments:\n",
    "    sentiment_text = ' '.join(df[df['sentiment'] == sentiment]['cleaned_input'].dropna())\n",
    "\n",
    "    if sentiment_text:\n",
    "        sentiment_words = sentiment_text.split()\n",
    "        sentiment_words_filtered = [word for word in sentiment_words\n",
    "                                   if word not in most_common_overall\n",
    "                                   and word not in stopwords_english]\n",
    "        filtered_sentiment_text = ' '.join(sentiment_words_filtered)\n",
    "\n",
    "        if filtered_sentiment_text:\n",
    "            wordcloud = WordCloud(width=800, height=400, background_color='white',\n",
    "                                stopwords=stopwords_english).generate(filtered_sentiment_text)\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.imshow(wordcloud, interpolation='bilinear')\n",
    "            plt.axis('off')\n",
    "            plt.title(f'Word Cloud for Sentiment: {sentiment} (excluding overall common words)')\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(f\"No significant words remaining for sentiment: {sentiment} after filtering.\")\n",
    "    else:\n",
    "        print(f\"No cleaned text available for sentiment: {sentiment}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6db7e2b1"
   },
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Additional Analysis\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Statistik deskriptif untuk setiap sentimen\n",
    "print(\"=\" * 80)\n",
    "print(\"STATISTIK DESKRIPTIF BERDASARKAN SENTIMEN\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "text_features = ['char_length', 'word_count', 'avg_word_length']\n",
    "sentiments = df['sentiment'].unique()\n",
    "\n",
    "descriptive_stats = {}\n",
    "\n",
    "for sentiment in sentiments:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Sentimen: {sentiment.upper()}\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    sentiment_df = df[df['sentiment'] == sentiment]\n",
    "    stats_dict = {}\n",
    "\n",
    "    for feature in text_features:\n",
    "        stats = {\n",
    "            'count': sentiment_df[feature].count(),\n",
    "            'mean': sentiment_df[feature].mean(),\n",
    "            'median': sentiment_df[feature].median(),\n",
    "            'std': sentiment_df[feature].std(),\n",
    "            'min': sentiment_df[feature].min(),\n",
    "            'max': sentiment_df[feature].max(),\n",
    "            'q25': sentiment_df[feature].quantile(0.25),\n",
    "            'q75': sentiment_df[feature].quantile(0.75)\n",
    "        }\n",
    "        stats_dict[feature] = stats\n",
    "\n",
    "    # Tampilkan dalam bentuk DataFrame untuk visualisasi yang lebih baik\n",
    "    stats_df = pd.DataFrame(stats_dict).T\n",
    "    print(f\"\\n{stats_df.round(2)}\")\n",
    "\n",
    "    descriptive_stats[sentiment] = stats_df\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STATISTIK DESKRIPTIF KESELURUHAN\")\n",
    "print(\"=\" * 80)\n",
    "overall_stats = df[text_features].describe()\n",
    "print(f\"\\n{overall_stats.round(2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Visualize distribution with Violin Plot\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, feature in enumerate(text_features):\n",
    "    sns.violinplot(data=df, x='sentiment', y=feature, ax=axes[idx], palette=sentiment_colors)\n",
    "    axes[idx].set_title(f'Distribusi {feature} berdasarkan Sentimen', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Sentimen', fontsize=10)\n",
    "    axes[idx].set_ylabel(feature, fontsize=10)\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize comparison of Mean and Median\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, feature in enumerate(text_features):\n",
    "    mean_values = [df[df['sentiment'] == s][feature].mean() for s in sentiments]\n",
    "    median_values = [df[df['sentiment'] == s][feature].median() for s in sentiments]\n",
    "\n",
    "    x = np.arange(len(sentiments))\n",
    "    width = 0.35\n",
    "\n",
    "    axes[idx].bar(x - width/2, mean_values, width, label='Mean', alpha=0.8)\n",
    "    axes[idx].bar(x + width/2, median_values, width, label='Median', alpha=0.8)\n",
    "\n",
    "    axes[idx].set_xlabel('Sentimen', fontsize=10)\n",
    "    axes[idx].set_ylabel('Nilai', fontsize=10)\n",
    "    axes[idx].set_title(f'Perbandingan Mean & Median {feature}', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xticks(x)\n",
    "    axes[idx].set_xticklabels(sentiments)\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Calculate overall correlation matrix\n",
    "correlation_matrix = df[text_features].corr()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MATRIKS KORELASI KESELURUHAN\")\n",
    "print(\"=\" * 60)\n",
    "print(correlation_matrix.round(3))\n",
    "print(\"\\n\")\n",
    "\n",
    "# Visualize overall correlation heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\n",
    "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8},\n",
    "            vmin=-1, vmax=1, fmt='.3f')\n",
    "plt.title('Matriks Korelasi Antar Variabel Numerik (Keseluruhan)',\n",
    "          fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze correlation per sentiment\n",
    "print(\"=\" * 60)\n",
    "print(\"MATRIKS KORELASI PER SENTIMEN\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, sentiment in enumerate(sentiments):\n",
    "    sentiment_df = df[df['sentiment'] == sentiment]\n",
    "    corr_matrix = sentiment_df[text_features].corr()\n",
    "\n",
    "    print(f\"\\nSentimen: {sentiment.upper()}\")\n",
    "    print(\"-\" * 60)\n",
    "    print(corr_matrix.round(3))\n",
    "\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0,\n",
    "                square=True, linewidths=1, ax=axes[idx],\n",
    "                vmin=-1, vmax=1, fmt='.3f', cbar_kws={\"shrink\": 0.8})\n",
    "    axes[idx].set_title(f'Korelasi - Sentimen {sentiment.capitalize()}',\n",
    "                       fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Encode sentiment as numeric values\n",
    "sentiment_encoding = {'positive': 1, 'neutral': 0, 'negative': -1}\n",
    "df['sentiment_encoded'] = df['sentiment'].map(sentiment_encoding)\n",
    "\n",
    "# Create correlation matrix between numerical features and sentiment\n",
    "correlation_features = ['char_length', 'word_count', 'avg_word_length', 'sentiment_encoded']\n",
    "correlation_matrix = df[correlation_features].corr()\n",
    "\n",
    "# Create heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\n",
    "      square=True, linewidths=2, cbar_kws={\"shrink\": 0.8},\n",
    "      vmin=-1, vmax=1, fmt='.3f', annot_kws={'size': 12, 'weight': 'bold'})\n",
    "\n",
    "plt.title('Matriks Korelasi antara Fitur Numerik dan Sentimen\\n(Positive=1, Neutral=0, Negative=-1)',\n",
    "      fontsize=14, fontweight='bold', pad=20)\n",
    "\n",
    "# Customize labels\n",
    "plt.xlabel('Fitur', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Fitur', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Rename labels for better readability - posisi di tengah cell\n",
    "labels = ['Panjang Karakter', 'Jumlah Kata', 'Rata-rata Panjang Kata', 'Sentimen']\n",
    "plt.xticks(np.arange(len(labels)) + 0.5, labels, rotation=45, ha='right')\n",
    "plt.yticks(np.arange(len(labels)) + 0.5, labels, rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print correlation values with sentiment\n",
    "print(\"=\" * 70)\n",
    "print(\"KORELASI ANTARA FITUR NUMERIK DAN SENTIMEN\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nKorelasi dengan Sentimen:\")\n",
    "print(f\"  - Panjang Karakter (char_length): {correlation_matrix.loc['char_length', 'sentiment_encoded']:.4f}\")\n",
    "print(f\"  - Jumlah Kata (word_count): {correlation_matrix.loc['word_count', 'sentiment_encoded']:.4f}\")\n",
    "print(f\"  - Rata-rata Panjang Kata (avg_word_length): {correlation_matrix.loc['avg_word_length', 'sentiment_encoded']:.4f}\")\n",
    "print(\"\\nInterpretasi:\")\n",
    "print(\"  Nilai mendekati 0 = Tidak ada korelasi linear\")\n",
    "print(\"  Nilai mendekati 1 = Korelasi positif kuat\")\n",
    "print(\"  Nilai mendekati -1 = Korelasi negatif kuat\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Scatter plot matrix to visualize relationships between variables\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# Character length vs word count\n",
    "for sentiment in sentiments:\n",
    "    sentiment_data = df[df['sentiment'] == sentiment]\n",
    "    axes[0, 0].scatter(sentiment_data['char_length'], sentiment_data['word_count'],\n",
    "                      label=sentiment, alpha=0.5, s=20, color=sentiment_colors[sentiment])\n",
    "axes[0, 0].set_xlabel('Character Length', fontsize=10)\n",
    "axes[0, 0].set_ylabel('Word Count', fontsize=10)\n",
    "axes[0, 0].set_title('Character Length vs Word Count', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# Character length vs average word length\n",
    "for sentiment in sentiments:\n",
    "    sentiment_data = df[df['sentiment'] == sentiment]\n",
    "    axes[0, 1].scatter(sentiment_data['char_length'], sentiment_data['avg_word_length'],\n",
    "                      label=sentiment, alpha=0.5, s=20, color=sentiment_colors[sentiment])\n",
    "axes[0, 1].set_xlabel('Character Length', fontsize=10)\n",
    "axes[0, 1].set_ylabel('Average Word Length', fontsize=10)\n",
    "axes[0, 1].set_title('Character Length vs Average Word Length', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# Word count vs average word length\n",
    "for sentiment in sentiments:\n",
    "    sentiment_data = df[df['sentiment'] == sentiment]\n",
    "    axes[1, 0].scatter(sentiment_data['word_count'], sentiment_data['avg_word_length'],\n",
    "                      label=sentiment, alpha=0.5, s=20, color=sentiment_colors[sentiment])\n",
    "axes[1, 0].set_xlabel('Word Count', fontsize=10)\n",
    "axes[1, 0].set_ylabel('Average Word Length', fontsize=10)\n",
    "axes[1, 0].set_title('Word Count vs Average Word Length', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(alpha=0.3)\n",
    "\n",
    "# Correlation insights\n",
    "axes[1, 1].axis('off')\n",
    "axes[1, 1].text(0.5, 0.5, 'Korelasi Insight:\\n\\n' +\n",
    "                '• char_length & word_count\\n  memiliki korelasi positif tinggi\\n\\n' +\n",
    "                '• avg_word_length relatif\\n  independen dari panjang teks',\n",
    "                ha='center', va='center', fontsize=11,\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Detection and Trend Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Calculate sentiment distribution and proportions\n",
    "sentiment_counts = df['sentiment'].value_counts()\n",
    "sentiment_proportions = df['sentiment'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DISTRIBUSI DAN PROPORSI SENTIMEN\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nJumlah Data per Sentimen:\")\n",
    "print(sentiment_counts)\n",
    "print(\"\\nProporsi (%) per Sentimen:\")\n",
    "print(sentiment_proportions.round(2))\n",
    "\n",
    "# Visualize proportions with pie chart and bar chart\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Pie Chart\n",
    "colors_list = [sentiment_colors[s] for s in sentiment_counts.index]\n",
    "axes[0].pie(sentiment_counts, labels=sentiment_counts.index, autopct='%1.1f%%',\n",
    "           startangle=90, colors=colors_list, explode=[0.05]*len(sentiment_counts))\n",
    "axes[0].set_title('Proporsi Sentimen dalam Dataset', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Bar Chart\n",
    "axes[1].bar(sentiment_counts.index, sentiment_counts.values,\n",
    "           color=[sentiment_colors[s] for s in sentiment_counts.index], alpha=0.8, edgecolor='black')\n",
    "axes[1].set_xlabel('Sentimen', fontsize=11)\n",
    "axes[1].set_ylabel('Jumlah Data', fontsize=11)\n",
    "axes[1].set_title('Distribusi Jumlah Data per Sentimen', fontsize=14, fontweight='bold')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "for i, (sentiment, count) in enumerate(zip(sentiment_counts.index, sentiment_counts.values)):\n",
    "    axes[1].text(i, count + 100, f'{count}\\n({sentiment_proportions[sentiment]:.1f}%)',\n",
    "                ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify outliers using IQR method\n",
    "for feature in text_features:\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    Q1 = df[feature].quantile(0.25)\n",
    "    Q3 = df[feature].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    outliers = df[(df[feature] < lower_bound) | (df[feature] > upper_bound)]\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Outlier Detection for {feature}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Q1: {Q1:.2f}, Q3: {Q3:.2f}, IQR: {IQR:.2f}\")\n",
    "    print(f\"Lower Bound: {lower_bound:.2f}, Upper Bound: {upper_bound:.2f}\")\n",
    "    print(f\"Number of Outliers: {len(outliers)} ({len(outliers)/len(df)*100:.2f}%)\")\n",
    "\n",
    "    sns.boxplot(data=df, x='sentiment', y=feature, palette=sentiment_colors)\n",
    "    plt.title(f'Outlier Detection - {feature}', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Sentiment')\n",
    "    plt.ylabel(feature)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Proportion Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics for each sentiment\n",
    "print(\"=\" * 80)\n",
    "print(\"STATISTIK DESKRIPTIF BERDASARKAN SENTIMEN\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "text_features = ['char_length', 'word_count', 'avg_word_length']\n",
    "sentiments = df['sentiment'].unique()\n",
    "\n",
    "for sentiment in sentiments:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Sentimen: {sentiment.upper()}\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    sentiment_df = df[df['sentiment'] == sentiment]\n",
    "    stats_dict = {}\n",
    "\n",
    "    for feature in text_features:\n",
    "        stats = {\n",
    "            'count': sentiment_df[feature].count(),\n",
    "            'mean': sentiment_df[feature].mean(),\n",
    "            'median': sentiment_df[feature].median(),\n",
    "            'std': sentiment_df[feature].std(),\n",
    "            'min': sentiment_df[feature].min(),\n",
    "            'max': sentiment_df[feature].max(),\n",
    "            'q25': sentiment_df[feature].quantile(0.25),\n",
    "            'q75': sentiment_df[feature].quantile(0.75)\n",
    "        }\n",
    "        stats_dict[feature] = stats\n",
    "\n",
    "    stats_df = pd.DataFrame(stats_dict).T\n",
    "    print(f\"\\n{stats_df.round(2)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"STATISTIK DESKRIPTIF KESELURUHAN\")\n",
    "print(\"=\" * 80)\n",
    "overall_stats = df[text_features].describe()\n",
    "print(f\"\\n{overall_stats.round(2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Compare text characteristics across sentiments\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, feature in enumerate(text_features):\n",
    "    sentiment_means = [df[df['sentiment'] == s][feature].mean() for s in sentiments]\n",
    "    sentiment_medians = [df[df['sentiment'] == s][feature].median() for s in sentiments]\n",
    "\n",
    "    x = np.arange(len(sentiments))\n",
    "    width = 0.35\n",
    "\n",
    "    bars1 = axes[idx].bar(x - width/2, sentiment_means, width, label='Mean', alpha=0.8, edgecolor='black')\n",
    "    bars2 = axes[idx].bar(x + width/2, sentiment_medians, width, label='Median', alpha=0.8, edgecolor='black')\n",
    "\n",
    "    for i, bar in enumerate(bars1):\n",
    "        bar.set_color(sentiment_colors[sentiments[i]])\n",
    "    for i, bar in enumerate(bars2):\n",
    "        bar.set_color(sentiment_colors[sentiments[i]])\n",
    "        bar.set_alpha(0.5)\n",
    "\n",
    "    axes[idx].set_xlabel('Sentimen', fontsize=11)\n",
    "    axes[idx].set_ylabel(f'{feature}', fontsize=11)\n",
    "    axes[idx].set_title(f'Perbandingan {feature} Antar Sentimen', fontsize=12, fontweight='bold')\n",
    "    axes[idx].set_xticks(x)\n",
    "    axes[idx].set_xticklabels(sentiments)\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary table of characteristics per sentiment\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RINGKASAN KARAKTERISTIK TEKS PER SENTIMEN\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "summary_data = []\n",
    "for sentiment in sentiments:\n",
    "    sentiment_df = df[df['sentiment'] == sentiment]\n",
    "    summary_data.append({\n",
    "        'Sentiment': sentiment,\n",
    "        'Count': len(sentiment_df),\n",
    "        'Proportion (%)': (len(sentiment_df) / len(df)) * 100,\n",
    "        'Avg Char Length': sentiment_df['char_length'].mean(),\n",
    "        'Avg Word Count': sentiment_df['word_count'].mean(),\n",
    "        'Avg Word Length': sentiment_df['avg_word_length'].mean()\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "display(summary_df.round(2))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "9471778b"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
